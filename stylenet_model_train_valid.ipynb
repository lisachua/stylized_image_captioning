{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "#from torch.autograd import Variable\n",
    "#from constant import get_symbol_id\n",
    "\n",
    "from collections import Counter,defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select 7k train images and 1k validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_7k_images(c_type='humor'):\n",
    "    '''8k -> 7k'''\n",
    "    # Create folder if does not exist.\n",
    "    if not os.path.exists('data/Flickr7k/'):\n",
    "        os.makedirs('data/Flickr7k/')\n",
    "    # open data/type/train.p\n",
    "    img_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )\n",
    "    \n",
    "    # copy imgs\n",
    "    for img_name in img_lst:\n",
    "        shutil.copyfile('data/Flicker8k_Dataset/' + img_name,\n",
    "                        'data/Flickr7k/' + img_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for first time \n",
    "select_7k_images(c_type='humor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_validation_images():\n",
    "    '''select test images randomly'''\n",
    "    # get filenames in flickr7k, 30k_images\n",
    "    filenames_7k = os.listdir('data/Flickr7k/')\n",
    "    filenames_8k = os.listdir('data/Flicker8k_Dataset')\n",
    "\n",
    "    filenames = list(set(filenames_8k) - set(filenames_7k))\n",
    "    print(\"img_num: \" + str(len(filenames)))\n",
    "\n",
    "    # copy images\n",
    "    validation_img_path = 'data/validation_images/'\n",
    "    if not os.path.exists(validation_img_path):\n",
    "        os.makedirs(validation_img_path)\n",
    "    for img_name in filenames:\n",
    "        shutil.copyfile('data/Flicker8k_Dataset/' + img_name,\n",
    "                        validation_img_path + img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for first time\n",
    "select_validation_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select 7k factual captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr8k_filename = \"data/Flickr8k_text/Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames in flickr7k_images\n",
    "filenames = os.listdir('data/Flickr7k/')\n",
    "# open factual caption: Flickr8k.token.txt\n",
    "with open(flickr8k_filename, 'r') as f:\n",
    "    res = f.readlines()\n",
    "\n",
    "# write out\n",
    "with open('data/factual_train.txt', 'w') as f:\n",
    "    r = re.compile(r'#\\d*')\n",
    "    for line in res:\n",
    "        img_id = r.split(line)[0]\n",
    "        if img_id in img_id_lst:\n",
    "            f.write(line)\n",
    "            \n",
    "with open('data/factual_valid.txt', 'w') as f:\n",
    "    r = re.compile(r'#\\d*')\n",
    "    for line in res:\n",
    "        img_id = r.split(line)[0]\n",
    "        if img_id not in img_id_lst:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_captions(mode='factual'):\n",
    "    ''' Extract captions from txt files to build vocabulary.'''\n",
    "    if mode == 'factual':\n",
    "        text = pd.read_csv(\"data/factual_train.txt\", \n",
    "                           encoding= 'unicode_escape', names=['line'])\n",
    "        text['caption'] = text['line'].str.split('\\t', n=1, expand=True)[1]\n",
    "\n",
    "    elif mode == 'humorous':\n",
    "        text = pd.read_csv(\"data/FlickrStyle_v0.9/humor/funny_train.txt\", \n",
    "                           encoding= 'unicode_escape', names=['caption'])\n",
    "    else:\n",
    "        text = pd.read_csv(\"data/FlickrStyle_v0.9/romantic/romantic_train.txt\", \n",
    "                           encoding= 'unicode_escape', names=['caption'])\n",
    "    return list(text.caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "def sub_br(x): return re_br.sub(\"\\n\", x)\n",
    "\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(sub_br(x))]\n",
    "\n",
    "def loadGloveModel(gloveFile=\"data/glove.6B.50d.txt\"):\n",
    "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
    "    f = open(gloveFile,'r')\n",
    "    word_vecs = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
    "    return word_vecs\n",
    "\n",
    "def get_vocab(mode_list=['factual', 'humorous']):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \n",
    "    Delete rare words from vocab if they are not in word_vecs\n",
    "    and don't have at least min_df occurrencies.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for mode in mode_list:\n",
    "        content.extend(extract_captions(mode))\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(spacy_tok(line))\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "def create_embedding_matrix(word_vecs, word_count, min_df=4, emb_size=50):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    #word_count = delete_rare_words(word_vecs, word_count, min_df)\n",
    "    V = len(word_count.keys()) + 2\n",
    "    vocab2index = {}\n",
    "    W = np.zeros((V, emb_size), dtype=\"float32\")\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    # adding a vector for padding\n",
    "    W[0] = np.zeros(emb_size, dtype='float32')\n",
    "    # adding a vector for rare words \n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "    vocab2index[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_count:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "    return W, np.array(vocab), vocab2index\n",
    "\n",
    "def encode_sentence_no_padding(s, vocab2index):\n",
    "    return np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = loadGloveModel()\n",
    "word_count = get_vocab(mode_list=['factual', 'humorous'])\n",
    "# word_count = delete_rare_words(word_vecs, word_count)\n",
    "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs, word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_and_caption(caption_file):\n",
    "    '''\n",
    "    Get image name and caption from factual caption file.\n",
    "    Returns array of tuples.\n",
    "    '''\n",
    "    text = pd.read_csv(caption_file, encoding= 'unicode_escape', names=['line'])\n",
    "    caption = text['line'].str.split('\\t', n=1, expand=True)[1]\n",
    "    img = text['line'].str.split('#', n=1, expand=True)[0]\n",
    "    return list(zip(img, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    im = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def resize_image(path, sz=224):\n",
    "    im = read_image(path)\n",
    "    return cv2.resize(im, (sz, sz))\n",
    "\n",
    "def normalize(im):\n",
    "    \"\"\"Normalizes images with Imagenet stats.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im - imagenet_stats[0])/imagenet_stats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr7kDataset(Dataset):\n",
    "    def __init__(self, img_dir, caption_file, vocab2index, transform=None):\n",
    "        '''\n",
    "        img_dir: Directory with all images.\n",
    "        caption_file: Path to factual caption file.\n",
    "        vocab2index: Vocab dictionary.\n",
    "        transform: Optional transforms to apply.\n",
    "        '''\n",
    "        self.img_dir = img_dir\n",
    "        self.img_caption_list = self._get_img_and_caption(caption_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def _get_img_and_caption(self, caption_file):\n",
    "        '''\n",
    "        Get image name and caption from factual caption file.\n",
    "        Returns array of tuples.\n",
    "        '''\n",
    "        text = pd.read_csv(caption_file, encoding= 'unicode_escape', names=['line'])\n",
    "        caption = text['line'].str.split('\\t', n=1, expand=True)[1]\n",
    "        img = text['line'].str.split('#', n=1, expand=True)[0]\n",
    "        return list(zip(img, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_caption_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns image and caption embedding.'''\n",
    "        img_name = self.img_caption_list[idx][0]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        if self.transform is not None:\n",
    "#             transform = transforms.Compose([Rescale((224, 224)),transforms.ToTensor()])\n",
    "#             image = transform(image)\n",
    "            image = resize_image(img_path, sz=224)\n",
    "            image = normalize(image)\n",
    "            image = np.rollaxis(image, 2)\n",
    "            \n",
    "        caption = self.img_caption_list[idx][1]\n",
    "        caption = encode_sentence_no_padding(caption, vocab2index)\n",
    "        \n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"data/Flickr7k\"\n",
    "cap_path = \"data/factual_train.txt\"\n",
    "flickr7k_ds = Flickr7kDataset(img_path, cap_path, vocab2index, transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, captions = flickr7k_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 224, 224)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    '''create minibatch tensors from data(list of tuple(image, caption))'''\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # images\n",
    "    images = [torch.Tensor(im) for im in images]\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # captions\n",
    "    captions = [torch.Tensor(c) for c in captions]\n",
    "    lengths = torch.LongTensor([len(c) for c in captions])\n",
    "    captions_padded = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return images, captions_padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, captions, lengths = collate_fn([flickr7k_ds[0], flickr7k_ds[1], flickr7k_ds[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([18]), tensor(18))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape, captions[0].shape, lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "flickr7k_dl = DataLoader(flickr7k_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_img_path = 'data/validation_images'\n",
    "valid_cap_path = \"data/factual_valid.txt\"\n",
    "valid_ds = Flickr7kDataset(valid_img_path, valid_cap_path, vocab2index, transform=True)\n",
    "valid_dl = DataLoader(flickr7k_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, captions, lengths = next(iter(flickr7k_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 224, 224]), torch.Size([3, 19]), torch.Size([3]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, captions.shape, lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrStyle7kDataset(Dataset):\n",
    "    def __init__(self, caption_file, vocab2index):\n",
    "        '''\n",
    "        caption_file: Path to styled caption file.\n",
    "        vocab2index: Vocab dictionary.\n",
    "        '''\n",
    "        self.caption_list = self._get_caption(caption_file)\n",
    "\n",
    "    def _get_caption(self, caption_file):\n",
    "        '''Get list of captions from styled caption file.'''\n",
    "        text = pd.read_csv(caption_file, encoding= 'unicode_escape', \n",
    "                           names=['caption'])\n",
    "        return list(text.caption)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.caption_list[idx]\n",
    "        caption = encode_sentence_no_padding(caption, vocab2index)\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_styled(captions):\n",
    "    # captions\n",
    "    captions = [torch.Tensor(c) for c in captions]\n",
    "    lengths = torch.LongTensor([len(c) for c in captions])\n",
    "    captions_padded = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "\n",
    "    return captions_padded, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size \n",
    "#shuffle = True\n",
    "cap_path_styled = \"data/FlickrStyle_v0.9/humor/funny_train.txt\"\n",
    "flickrstyle7k_ds = FlickrStyle7kDataset(cap_path_styled, vocab2index)\n",
    "\n",
    "flickrstyle7k_dl = DataLoader(dataset=flickrstyle7k_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn_styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions, lengths = next(iter(flickrstyle7k_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 17]), torch.Size([3]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.shape, lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (images, captions, lengths) in enumerate(flickr7k_dl):\n",
    "    print(i)\n",
    "    print(images.shape)\n",
    "    print(captions.shape)\n",
    "    print(lengths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    if torch.cuda.is_available():\n",
    "        length = length.cuda()\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "         False, False, False]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "length = torch.LongTensor([23, 21, 17])\n",
    "\n",
    "print(sequence_mask(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        '''\n",
    "        Load the pretrained ResNet152 and replace fc\n",
    "        '''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.A = nn.Linear(resnet.fc.in_features, emb_dim)\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''Extract the image feature vectors'''\n",
    "        features = self.resnet(images)\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.A(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactoredLSTM(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, factored_dim,  vocab_size):\n",
    "        super(FactoredLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # embedding\n",
    "        self.B = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        # factored lstm weights\n",
    "        self.U_i = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fi = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_i = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_i = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_f = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_ff = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_f = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_f = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_o = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fo = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_o = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_c = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fc = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_c = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_c = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # h - humorous\n",
    "        self.S_hi = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_hf = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_ho = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_hc = nn.Linear(factored_dim, factored_dim)\n",
    "\n",
    "        # r - romantic\n",
    "        # self.S_ri = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_rf = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_ro = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_rc = nn.Linear(factored_dim, factored_dim)\n",
    "\n",
    "        # weight for output\n",
    "        self.C = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward_step(self, embedded, h_0, c_0, mode):\n",
    "        # transform embedded from emb_dim --> factored_dim\n",
    "        i = self.V_i(embedded)\n",
    "        f = self.V_f(embedded)\n",
    "        o = self.V_o(embedded)\n",
    "        c = self.V_c(embedded)\n",
    "        \n",
    "        # factored_dim --> factored_dim\n",
    "        if mode == \"factual\":\n",
    "            i = self.S_fi(i)\n",
    "            f = self.S_ff(f)\n",
    "            o = self.S_fo(o)\n",
    "            c = self.S_fc(c)\n",
    "        elif mode == \"humorous\":\n",
    "            i = self.S_hi(i)\n",
    "            f = self.S_hf(f)\n",
    "            o = self.S_ho(o)\n",
    "            c = self.S_hc(c)\n",
    "        # elif mode == \"romantic\":\n",
    "        #     i = self.S_ri(i)\n",
    "        #     f = self.S_rf(f)\n",
    "        #     o = self.S_ro(o)\n",
    "        #     c = self.S_rc(c)\n",
    "        else:\n",
    "            sys.stderr.write(\"mode name wrong!\")\n",
    "\n",
    "        i_t = F.sigmoid(self.U_i(i.double()) + self.W_i(h_0.double()))\n",
    "        f_t = F.sigmoid(self.U_f(f.double()) + self.W_f(h_0.double()))\n",
    "        o_t = F.sigmoid(self.U_o(o.double()) + self.W_o(h_0.double()))\n",
    "        c_tilda = F.tanh(self.U_c(c.double()) + self.W_c(h_0.double()))\n",
    "\n",
    "        c_t = f_t * c_0 + i_t * c_tilda\n",
    "        h_t = o_t * c_t\n",
    "\n",
    "        outputs = self.C(h_t)\n",
    "\n",
    "        return outputs, h_t, c_t\n",
    "\n",
    "    def forward(self, captions, features=None, mode=\"factual\"):\n",
    "        '''\n",
    "        Args:\n",
    "            features: fixed vectors from images, [batch, emb_dim]\n",
    "            captions: [batch, max_len]\n",
    "            mode: type of caption to generate\n",
    "        '''\n",
    "        batch_size = captions.size(0)\n",
    "        embedded = self.B(captions)  # [batch, max_len, emb_dim]\n",
    "        # concat image features and captions\n",
    "        if mode == \"factual\":\n",
    "            if features is None:\n",
    "                sys.stderr.write(\"features is None!\")\n",
    "            embedded = torch.cat((features.unsqueeze(1), embedded), 1)\n",
    "\n",
    "        # initialize hidden state\n",
    "        h_t = torch.Tensor(batch_size, self.hidden_dim)\n",
    "        c_t = torch.Tensor(batch_size, self.hidden_dim)\n",
    "        nn.init.uniform(h_t)\n",
    "        nn.init.uniform(c_t)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h_t = h_t.cuda()\n",
    "            c_t = c_t.cuda()\n",
    "\n",
    "        all_outputs = []\n",
    "        # iterate\n",
    "        for ix in range(embedded.size(1) - 1):\n",
    "            emb = embedded[:, ix, :]\n",
    "            outputs, h_t, c_t = self.forward_step(emb, h_t, c_t, mode=mode)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "        all_outputs = torch.stack(all_outputs, 1)\n",
    "\n",
    "        return all_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# def eval_outputs(outputs, vocab):\n",
    "#     # outputs: [batch, max_len - 1, vocab_size]\n",
    "#     indices = torch.topk(outputs, 1)[1]\n",
    "#     indices = indices.squeeze(2)\n",
    "#     indices = indices.data\n",
    "#     for i in range(len(indices)):\n",
    "#         caption = [vocab.i2w[x] for x in indices[i]]\n",
    "#         print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "# data_loader = get_data_loader(img_path, cap_path, vocab, batch_size)\n",
    "# styled_data_loader = get_styled_data_loader(cap_path_styled, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, vocab2index.items()))\n",
    "reverse_word_map[0]='<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_map[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(encoder, decoder, valid_fact_dl):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total = 0\n",
    "    sum_bleu = 0\n",
    "    sum_loss = 0\n",
    "    for n,(images, captions, lengths) in enumerate(valid_fact_dl):\n",
    "        batch = length.shape[0]\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            captions = captions.cuda()\n",
    "        features = encoder(images.double())\n",
    "        outputs = decoder(captions.long(), features.double(), mode=\"factual\")\n",
    "        loss = criterion(outputs[:, 1:, :].contiguous(),\n",
    "                             captions[:, 1:].contiguous().long(), lengths - 1)\n",
    "        sum_loss += loss.data.mean()\n",
    "        \n",
    "        indices = torch.topk(outputs, 1)[1].squeeze(2).data\n",
    "        for i in range(batch):\n",
    "            predicted_caption = [reverse_word_map[x.item()] for x in indices[i]]\n",
    "            actual_caption = [reverse_word_map[x.item()] for x in captions[i]]\n",
    "            #print(predicted_caption, actual_caption)\n",
    "            bleu_score = corpus_bleu(predicted_caption, actual_caption)\n",
    "            print(bleu_score)\n",
    "            sum_bleu += bleu_score\n",
    "            total += 1\n",
    "        if n==3: break\n",
    "    return sum_loss/total, sum_bleu/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, \n",
    "          train_fact_dl, valid_fact_dl, style_dl, \n",
    "          epoch_num, optimizer_cap, optimizer_lang,\n",
    "          total_cap_step, total_lang_step, model_path):\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        # caption\n",
    "        for i, (images, captions, lengths) in enumerate(train_fact_dl):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                captions = captions.cuda()\n",
    "\n",
    "            # forward, backward and optimize\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            features = encoder(images.double())\n",
    "            outputs = decoder(captions.long(), features.double(), mode=\"factual\")\n",
    "            loss = criterion(outputs[:, 1:, :].contiguous(),\n",
    "                             captions[:, 1:].contiguous().long(), lengths - 1)\n",
    "            loss.backward()\n",
    "            optimizer_cap.step()\n",
    "\n",
    "            # print log\n",
    "#             if i % 50 == 0:\n",
    "#                 print(\"Epoch [%d/%d], CAP, Step [%d/%d], Loss: %.4f\"\n",
    "#                       % (epoch+1, epoch_num, i, total_cap_step,\n",
    "#                           loss.data.mean()))\n",
    "            if i==2: break\n",
    "            print(\"Epoch [%d/%d], CAP, Step [%d/%d], Loss: %.4f\"\n",
    "                      % (epoch+1, epoch_num, i, total_cap_step,\n",
    "                          loss.data.mean()))\n",
    "\n",
    "        #eval_outputs(outputs, vocab)\n",
    "        val_loss, val_bleu = val_metrics(encoder, decoder, valid_fact_dl)\n",
    "        print(\"val_loss %.3f val_bleu %.3f\" % (val_loss, val_bleu))\n",
    "\n",
    "        # language\n",
    "        for i, (captions, lengths) in enumerate(style_dl):\n",
    "            if torch.cuda.is_available():\n",
    "                captions = captions.cuda()\n",
    "\n",
    "            # forward, backward and optimize\n",
    "            decoder.zero_grad()\n",
    "            outputs = decoder(captions.long(), mode='humorous')\n",
    "            loss = criterion(outputs, captions[:, 1:].contiguous().long(), lengths-1)\n",
    "            loss.backward()\n",
    "            optimizer_lang.step()\n",
    "\n",
    "            # print log\n",
    "#             if i % 10 == 0:\n",
    "#                 print(\"Epoch [%d/%d], LANG, Step [%d/%d], Loss: %.4f\"\n",
    "#                       % (epoch+1, epoch_num, i, total_lang_step,\n",
    "#                           loss.data.mean()))\n",
    "            if i==3: break\n",
    "            print(\"Epoch [%d/%d], LANG, Step [%d/%d], Loss: %.4f\"\n",
    "                      % (epoch+1, epoch_num, i, total_lang_step,\n",
    "                          loss.data.mean()))\n",
    "\n",
    "        # save models\n",
    "        torch.save(decoder.state_dict(),\n",
    "                   os.path.join(model_path, 'decoder-%d.pkl' % (epoch + 1,)))\n",
    "\n",
    "        torch.save(encoder.state_dict(),\n",
    "                   os.path.join(model_path, 'encoder-%d.pkl' % (epoch + 1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "flickr7k_dl = DataLoader(flickr7k_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn)\n",
    "flickrstyle7k_dl = DataLoader(dataset=flickrstyle7k_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn_styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "hidden_dim = 512\n",
    "factored_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "encoder = EncoderCNN(emb_dim)\n",
    "decoder = FactoredLSTM(emb_dim, hidden_dim, factored_dim, vocab_size)\n",
    "\n",
    "encoder = encoder.double()\n",
    "decoder = decoder.double()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "total_cap_step = len(flickr7k_dl)\n",
    "total_lang_step = len(flickrstyle7k_dl)\n",
    "epoch_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "lr_caption = 0.0002\n",
    "lr_language = 0.0005\n",
    "\n",
    "criterion = masked_cross_entropy\n",
    "cap_params = list(decoder.parameters()) + list(encoder.A.parameters())\n",
    "lang_params = list(decoder.parameters())\n",
    "optimizer_cap = torch.optim.Adam(cap_params, lr=lr_caption)\n",
    "optimizer_lang = torch.optim.Adam(lang_params, lr=lr_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'pretrained_models'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:102: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:103: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], CAP, Step [0/11667], Loss: 11.0237\n",
      "Epoch [1/1], CAP, Step [1/11667], Loss: 8.3756\n",
      "['in', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['Woman', 'in', 'a', 'black', 'dress', 'walking', 'on', 'the', 'street', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.788429383461836e-232\n",
      "['in', 'dog', 'in', 'in', 'dog', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['A', 'woman', 'holds', 'a', 'baby', 'while', 'a', 'boy', 'sits', 'next', 'to', 'her', 'smiling', '.', '<PAD>', '<PAD>', '<PAD>']\n",
      "7.752349855101862e-232\n",
      "['in', 'dog', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['A', 'man', 'is', 'sitting', 'on', 'the', 'floor', 'outside', 'a', 'door', 'and', 'his', 'head', 'on', 'his', 'chin', '.']\n",
      "9.460904289886527e-232\n",
      "['dog', 'dog', 'dog', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['A', 'cricket', 'bowler', 'in', 'action', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "8.972141065609098e-232\n",
      "['in', 'dog', 'dog', 'a', 'dog', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['A', 'black', 'and', 'a', 'blonde', 'dog', 'are', 'either', 'playing', 'or', 'fighting', 'with', 'each', 'other', '.']\n",
      "1.0692267874642598e-231\n",
      "['in', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['Black', 'and', 'brown', 'dog', 'with', 'red', 'harness', 'in', 'high', 'grass', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "8.54457989334043e-232\n",
      "['in', 'dog', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['A', 'dog', 'in', 'a', 'field', 'with', 'a', 'Frisbee', '.', '<PAD>']\n",
      "1.2662006142931263e-231\n",
      "['in', 'in', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['Two', 'boys', 'eating', 'UNK', 'outside', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "8.412065649527267e-232\n",
      "['in', 'in', 'in', 'in', 'a', 'a', 'a', 'a', 'a', 'a'] ['a', 'UNK', 'hound', 'dog', 'walking', 'through', 'the', 'grass', 'outside', '.']\n",
      "9.418382295637229e-232\n",
      "['in', 'in', 'in', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['Blonde', 'lady', 'looks', 'at', 'Space', 'Needle', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "8.029654430073048e-232\n",
      "['in', 'dog', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'] ['A', 'dark', 'dog', 'is', 'competing', 'in', 'a', 'dog', 'show', 'competition', '.']\n",
      "8.561894227089738e-232\n",
      "['in', 'dog', 'in', 'in', 'in', 'a', 'a', 'a', 'a', 'a', 'a'] ['A', 'motorcyclist', 'pops', 'a', 'wheelie', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "8.107244643554327e-232\n",
      "val_loss 2.470 val_bleu 0.000\n",
      "Epoch [1/1], LANG, Step [0/2334], Loss: 8.4016\n",
      "Epoch [1/1], LANG, Step [1/2334], Loss: 8.2566\n",
      "Epoch [1/1], LANG, Step [2/2334], Loss: 8.0113\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, \n",
    "      flickr7k_dl, valid_dl, flickrstyle7k_dl, \n",
    "      epoch_num, optimizer_cap, optimizer_lang,\n",
    "      total_cap_step, total_lang_step,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
