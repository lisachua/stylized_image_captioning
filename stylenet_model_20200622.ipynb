{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_7k_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_7k_images(c_type='humor'):\n",
    "    '''8k -> 7k'''\n",
    "    # open data/type/train.p\n",
    "    img_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )\n",
    "    \n",
    "    # copy imgs\n",
    "    for img_name in img_lst:\n",
    "        shutil.copyfile('data/Flicker8k_Dataset/' + img_name,\n",
    "                        'data/Flickr7k/' + img_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_7k_images(c_type='humor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_factual_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr8k_filename = \"data/Flickr8k_text/Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames in flickr7k_images\n",
    "filenames = os.listdir('data/Flickr7k/')\n",
    "# open factual caption: Flickr8k.token.txt\n",
    "with open(flickr8k_filename, 'r') as f:\n",
    "    res = f.readlines()\n",
    "\n",
    "# write out\n",
    "with open('data/factual_train.txt', 'w') as f:\n",
    "    r = re.compile(r'#\\d*')\n",
    "    for line in res:\n",
    "        img_id = r.split(line)[0]\n",
    "        if img_id in img_id_lst:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random_select_test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_select_test_images(num=100):\n",
    "    '''select test images randomly'''\n",
    "    # get filenames in flickr7k, 30k_images\n",
    "    filenames_7k = os.listdir('data/Flickr7k/')\n",
    "    filenames_30k = os.listdir('data/Flicker8k_Dataset')\n",
    "\n",
    "    filenames = list(set(filenames_30k) - set(filenames_7k))\n",
    "    print(\"img_num: \" + str(len(filenames)))\n",
    "    random.seed(24)\n",
    "    selected = random.sample(filenames, num)\n",
    "\n",
    "    # copy images\n",
    "    for img_name in selected:\n",
    "        shutil.copyfile('data/Flicker8k_Dataset/' + img_name,\n",
    "                        'data/test_images/' + img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_num: 1091\n"
     ]
    }
   ],
   "source": [
    "random_select_test_images(num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocab:\n",
    "    '''vocabulary'''\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.ix = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.w2i:\n",
    "            self.w2i[word] = self.ix\n",
    "            self.i2w[self.ix] = word\n",
    "            self.ix += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.w2i:\n",
    "            return self.w2i['<unk>']\n",
    "        return self.w2i[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(mode_list=['factual', 'humorous']):\n",
    "    '''build vocabulary'''\n",
    "    # define vocabulary\n",
    "    vocab = Vocab()\n",
    "    # add special tokens\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<s>')\n",
    "    vocab.add_word('</s>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # add words\n",
    "    for mode in mode_list:\n",
    "        if mode == 'factual':\n",
    "            captions = extract_captions(mode=mode)\n",
    "            words = nltk.tokenize.word_tokenize(captions)\n",
    "            counter = Counter(words)\n",
    "            words = [word for word, cnt in counter.items() if cnt >= 2]\n",
    "        else:\n",
    "            captions = extract_captions(mode=mode)\n",
    "            words = nltk.tokenize.word_tokenize(captions)\n",
    "\n",
    "        for word in words:\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/factual_train.txt\", 'r') as f:\n",
    "    res = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_captions(mode='factual'):\n",
    "    '''extract captions from data files for building vocabulary'''\n",
    "    text = ''\n",
    "    if mode == 'factual':\n",
    "        with open(\"data/factual_train.txt\", 'r') as f:\n",
    "            res = f.readlines()\n",
    "\n",
    "    elif mode == 'humorous':\n",
    "        with open(\"data/FlickrStyle_v0.9/humor/funny_train.txt\", 'r') as f:\n",
    "            res = f.readlines()\n",
    "    else:\n",
    "        with open(\"data/FlickrStyle_v0.9/romantic/romantic_train.txt\", 'r') as f:\n",
    "            res = f.readlines()\n",
    "\n",
    "    for line in res:\n",
    "        line = line.replace('.', '')\n",
    "        line = line.strip()\n",
    "        text += line + ' '\n",
    "\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6856\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(mode_list=['factual', 'humorous'])\n",
    "print(vocab.__len__())\n",
    "with open('data/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flickr7kDataset(Dataset):\n",
    "    '''Flickr7k dataset'''\n",
    "    def __init__(self, img_dir, caption_file, vocab, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            img_dir: Direcutory with all the images\n",
    "            caption_file: Path to the factual caption file\n",
    "            vocab: Vocab instance\n",
    "            transform: Optional transform to be applied\n",
    "        '''\n",
    "        self.img_dir = img_dir\n",
    "        self.imgname_caption_list = self._get_imgname_and_caption(caption_file)\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def _get_imgname_and_caption(self, caption_file):\n",
    "        '''extract image name and caption from factual caption file'''\n",
    "        with open(caption_file, 'r') as f:\n",
    "            res = f.readlines()\n",
    "\n",
    "        imgname_caption_list = []\n",
    "        r = re.compile(r'#\\d*')\n",
    "        for line in res:\n",
    "            img_and_cap = r.split(line)\n",
    "            img_and_cap = [x.strip() for x in img_and_cap]\n",
    "            imgname_caption_list.append(img_and_cap)\n",
    "\n",
    "        return imgname_caption_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgname_caption_list)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        '''return one data pair (image and captioin)'''\n",
    "        img_name = self.imgname_caption_list[ix][0]\n",
    "        img_name = os.path.join(self.img_dir, img_name)\n",
    "        caption = self.imgname_caption_list[ix][1]\n",
    "\n",
    "        image = skimage.io.imread(img_name)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # convert caption to word ids\n",
    "        r = re.compile(\"\\.\")\n",
    "        tokens = nltk.tokenize.word_tokenize(r.sub(\"\", caption).lower())\n",
    "        caption = []\n",
    "        caption.append(self.vocab('<s>'))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab('</s>'))\n",
    "        caption = torch.Tensor(caption)\n",
    "        return image, caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(data):\n",
    "    '''create minibatch tensors from data(list of tuple(image, caption))'''\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # images : tuple of 3D tensor -> 4D tensor\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # captions : tuple of 1D Tensor -> 2D tensor\n",
    "    lengths = torch.LongTensor([len(cap) for cap in captions])\n",
    "    print(lengths.shape)\n",
    "    captions = [pad_sequence_dl(cap, max(lengths)) for cap in captions]\n",
    "    captions = torch.stack(captions, 0)\n",
    "\n",
    "    return images, captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_sequence_dl(seq, max_len):\n",
    "    seq = torch.cat((seq, torch.zeros(max_len - len(seq))))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loader(img_dir, caption_file, vocab, batch_size,\n",
    "                    transform=None, shuffle=False, num_workers=0):\n",
    "    '''Return data_loader'''\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            Rescale((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    flickr7k = Flickr7kDataset(img_dir, caption_file, vocab, transform)\n",
    "\n",
    "    data_loader = DataLoader(dataset=flickr7k,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers,\n",
    "                             collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FlickrStyle7kDataset(Dataset):\n",
    "    '''Styled caption dataset'''\n",
    "    def __init__(self, caption_file, vocab):\n",
    "        '''\n",
    "        Args:\n",
    "            caption_file: Path to styled caption file\n",
    "            vocab: Vocab instance\n",
    "        '''\n",
    "        self.caption_list = self._get_caption(caption_file)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def _get_caption(self, caption_file):\n",
    "        '''extract caption list from styled caption file'''\n",
    "        with open(caption_file, 'r') as f:\n",
    "            caption_list = f.readlines()\n",
    "\n",
    "        caption_list = [x.strip() for x in caption_list]\n",
    "        return caption_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_list)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        caption = self.caption_list[ix]\n",
    "        # convert caption to word ids\n",
    "        r = re.compile(\"\\.\")\n",
    "        tokens = nltk.tokenize.word_tokenize(r.sub(\"\", caption).lower())\n",
    "        caption = []\n",
    "        caption.append(self.vocab('<s>'))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab('</s>'))\n",
    "        caption = torch.Tensor(caption)\n",
    "        return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def collate_fn_styled(captions):\n",
    "    captions.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "    # tuple of 1D Tensor -> 2D Tensor\n",
    "    lengths = torch.LongTensor([len(cap) for cap in captions])\n",
    "    captions = [pad_sequence_dl(cap, max(lengths)) for cap in captions]\n",
    "    captions = torch.stack(captions, 0)\n",
    "\n",
    "    return captions, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_styled_data_loader(caption_file, vocab, batch_size,\n",
    "                           shuffle=False, num_workers=0):\n",
    "    '''Return data_loader for styled caption'''\n",
    "    flickr_styled_7k = FlickrStyle7kDataset(caption_file, vocab)\n",
    "\n",
    "    data_loader = DataLoader(dataset=flickr_styled_7k,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers,\n",
    "                             collate_fn=collate_fn_styled)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Rescale:\n",
    "    '''Rescale the image to a given size\n",
    "    Args:\n",
    "        output_size(int or tuple)\n",
    "    '''\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        image = skimage.transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data/vocab.pkl\", 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "img_path = \"data/Flickr7k\"\n",
    "cap_path = \"data/factual_train.txt\"\n",
    "cap_path_styled = \"data/FlickrStyle_v0.9/humor/funny_train.txt\"\n",
    "data_loader = get_data_loader(img_path, cap_path, vocab, 3)\n",
    "styled_data_loader = get_styled_data_loader(cap_path_styled, vocab, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[1.1900e+02, 1.2100e+02, 5.9500e+02, 5.8400e+02, 4.2800e+02, 4.0700e+02,\n",
      "         6.1000e+01, 6.7900e+02, 2.9700e+02, 6.0000e+00, 2.1660e+03, 1.3000e+01,\n",
      "         1.0270e+03, 2.1670e+03, 2.0000e+00],\n",
      "        [4.0000e+00, 1.8000e+01, 3.1000e+01, 4.7000e+01, 7.0000e+00, 6.0000e+00,\n",
      "         5.8000e+01, 1.2800e+02, 9.0000e+00, 1.5800e+02, 7.3500e+02, 6.7000e+01,\n",
      "         1.5000e+01, 2.1700e+03, 2.0000e+00],\n",
      "        [4.0000e+00, 2.3000e+01, 2.4000e+01, 2.3600e+02, 1.8120e+03, 6.6000e+01,\n",
      "         6.7000e+01, 4.0000e+00, 1.1000e+03, 6.7000e+01, 2.1680e+03, 3.2000e+01,\n",
      "         2.1690e+03, 2.0000e+00, 0.0000e+00]])\n",
      "tensor([15, 15, 14])\n",
      "\n",
      "1\n",
      "tensor([[4.0000e+00, 2.4000e+01, 1.9800e+02, 4.0000e+00, 2.3100e+02, 6.0000e+00,\n",
      "         6.1000e+01, 2.2200e+02, 4.9400e+02, 6.7000e+01, 2.1740e+03, 3.2000e+01,\n",
      "         2.1750e+03, 2.1760e+03, 2.0000e+00],\n",
      "        [4.0000e+00, 1.8000e+01, 1.3100e+02, 3.7000e+01, 6.1000e+01, 3.5000e+02,\n",
      "         6.0000e+01, 6.1000e+01, 8.9000e+01, 6.3000e+01, 2.1710e+03, 1.6400e+02,\n",
      "         2.1720e+03, 2.0000e+00, 0.0000e+00],\n",
      "        [4.0000e+00, 1.8000e+01, 9.0000e+00, 2.1730e+03, 4.1000e+01, 4.0000e+00,\n",
      "         6.4000e+01, 3.3500e+02, 9.8700e+02, 6.0000e+00, 6.1000e+01, 1.0100e+02,\n",
      "         2.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([15, 14, 13])\n",
      "\n",
      "2\n",
      "tensor([[4.0000e+00, 6.9000e+01, 1.8800e+02, 4.0000e+00, 6.4000e+01, 7.6700e+02,\n",
      "         9.0000e+00, 5.2000e+01, 7.8000e+01, 6.1000e+01, 8.3100e+02, 6.3000e+01,\n",
      "         4.9400e+02, 6.7000e+01, 2.1780e+03, 2.0000e+00],\n",
      "        [4.0000e+00, 4.8100e+02, 3.6400e+02, 2.4000e+01, 6.0000e+00, 2.0530e+03,\n",
      "         9.0000e+00, 4.8000e+01, 6.5000e+01, 3.1000e+01, 4.0000e+00, 7.5400e+02,\n",
      "         1.3000e+01, 1.9580e+03, 2.0000e+00, 0.0000e+00],\n",
      "        [4.0000e+00, 3.5000e+01, 3.8300e+02, 1.6400e+02, 6.1000e+01, 1.6500e+02,\n",
      "         6.3000e+01, 4.8100e+02, 3.1000e+01, 4.0000e+00, 6.2600e+02, 2.1770e+03,\n",
      "         2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([16, 15, 13])\n",
      "\n",
      "3\n",
      "tensor([[4.0000e+00, 8.2000e+01, 3.2200e+02, 2.0400e+02, 6.0000e+00, 6.1000e+01,\n",
      "         1.0100e+02, 1.3320e+03, 4.0000e+00, 2.1790e+03, 3.1000e+01, 4.0000e+00,\n",
      "         1.5600e+02, 1.3000e+01, 6.1000e+01, 2.1300e+02, 2.0000e+00],\n",
      "        [4.0000e+00, 1.0400e+02, 6.0000e+00, 4.0000e+00, 6.4000e+01, 2.8100e+02,\n",
      "         9.0000e+00, 2.1800e+03, 4.0000e+00, 2.1810e+03, 5.2700e+02, 4.9400e+02,\n",
      "         6.7000e+01, 9.4000e+01, 4.0000e+00, 2.1820e+03, 2.0000e+00],\n",
      "        [4.0000e+00, 1.7900e+02, 1.8500e+02, 1.3000e+01, 1.1780e+03, 2.6800e+02,\n",
      "         6.0000e+00, 4.0000e+00, 1.9330e+03, 1.3320e+03, 4.0000e+00, 2.1830e+03,\n",
      "         1.3000e+01, 1.4880e+03, 2.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([17, 17, 15])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, (captions, lengths) in enumerate(styled_data_loader):\n",
    "    print(i)\n",
    "    # print(images.shape)\n",
    "    print(captions[:, 1:])\n",
    "    print(lengths - 1)\n",
    "    print()\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(length)\n",
    "    if torch.cuda.is_available():\n",
    "        length = length.cuda()\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "         False, False, False]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "length = torch.LongTensor([23, 21, 17])\n",
    "length = Variable(length)\n",
    "\n",
    "print(sequence_mask(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_symbol_id' from 'constant' (/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/constant/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-4a861c00c3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_symbol_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_symbol_id' from 'constant' (/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/constant/__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from constant import get_symbol_id\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        '''\n",
    "        Load the pretrained ResNet152 and replace fc\n",
    "        '''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.A = nn.Linear(resnet.fc.in_features, emb_dim)\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''Extract the image feature vectors'''\n",
    "        features = self.resnet(images)\n",
    "        features = Variable(features.data)\n",
    "        # if torch.cuda.is_available():\n",
    "        #     features = features.cuda()\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.A(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class FactoredLSTM(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, factored_dim,  vocab_size):\n",
    "        super(FactoredLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # embedding\n",
    "        self.B = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        # factored lstm weights\n",
    "        self.U_i = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fi = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_i = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_i = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_f = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_ff = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_f = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_f = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_o = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fo = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_o = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_c = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fc = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_c = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_c = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.S_hi = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_hf = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_ho = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_hc = nn.Linear(factored_dim, factored_dim)\n",
    "\n",
    "        # self.S_ri = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_rf = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_ro = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_rc = nn.Linear(factored_dim, factored_dim)\n",
    "\n",
    "        # weight for output\n",
    "        self.C = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward_step(self, embedded, h_0, c_0, mode):\n",
    "        i = self.V_i(embedded)\n",
    "        f = self.V_f(embedded)\n",
    "        o = self.V_o(embedded)\n",
    "        c = self.V_c(embedded)\n",
    "\n",
    "        if mode == \"factual\":\n",
    "            i = self.S_fi(i)\n",
    "            f = self.S_ff(f)\n",
    "            o = self.S_fo(o)\n",
    "            c = self.S_fc(c)\n",
    "        elif mode == \"humorous\":\n",
    "            i = self.S_hi(i)\n",
    "            f = self.S_hf(f)\n",
    "            o = self.S_ho(o)\n",
    "            c = self.S_hc(c)\n",
    "        # elif mode == \"romantic\":\n",
    "        #     i = self.S_ri(i)\n",
    "        #     f = self.S_rf(f)\n",
    "        #     o = self.S_ro(o)\n",
    "        #     c = self.S_rc(c)\n",
    "        else:\n",
    "            sys.stderr.write(\"mode name wrong!\")\n",
    "\n",
    "        i_t = F.sigmoid(self.U_i(i) + self.W_i(h_0))\n",
    "        f_t = F.sigmoid(self.U_f(f) + self.W_f(h_0))\n",
    "        o_t = F.sigmoid(self.U_o(o) + self.W_o(h_0))\n",
    "        c_tilda = F.tanh(self.U_c(c) + self.W_c(h_0))\n",
    "\n",
    "        c_t = f_t * c_0 + i_t * c_tilda\n",
    "        h_t = o_t * c_t\n",
    "\n",
    "        outputs = self.C(h_t)\n",
    "\n",
    "        return outputs, h_t, c_t\n",
    "\n",
    "    def forward(self, captions, features=None, mode=\"factual\"):\n",
    "        '''\n",
    "        Args:\n",
    "            features: fixed vectors from images, [batch, emb_dim]\n",
    "            captions: [batch, max_len]\n",
    "            mode: type of caption to generate\n",
    "        '''\n",
    "        batch_size = captions.size(0)\n",
    "        embedded = self.B(captions)  # [batch, max_len, emb_dim]\n",
    "        # concat features and captions\n",
    "        if mode == \"factual\":\n",
    "            if features is None:\n",
    "                sys.stderr.write(\"features is None!\")\n",
    "            embedded = torch.cat((features.unsqueeze(1), embedded), 1)\n",
    "\n",
    "        # initialize hidden state\n",
    "        h_t = Variable(torch.Tensor(batch_size, self.hidden_dim))\n",
    "        c_t = Variable(torch.Tensor(batch_size, self.hidden_dim))\n",
    "        nn.init.uniform(h_t)\n",
    "        nn.init.uniform(c_t)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h_t = h_t.cuda()\n",
    "            c_t = c_t.cuda()\n",
    "\n",
    "        all_outputs = []\n",
    "        # iterate\n",
    "        for ix in range(embedded.size(1) - 1):\n",
    "            emb = embedded[:, ix, :]\n",
    "            outputs, h_t, c_t = self.forward_step(emb, h_t, c_t, mode=mode)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "        all_outputs = torch.stack(all_outputs, 1)\n",
    "\n",
    "        return all_outputs\n",
    "\n",
    "    def sample(self, feature, beam_size=5, max_len=30, mode=\"factual\"):\n",
    "        '''\n",
    "        generate captions from feature vectors with beam search\n",
    "\n",
    "        Args:\n",
    "            features: fixed vector for an image, [1, emb_dim]\n",
    "            beam_size: stock size for beam search\n",
    "            max_len: max sampling length\n",
    "            mode: type of caption to generate\n",
    "        '''\n",
    "        # initialize hidden state\n",
    "        h_t = Variable(torch.Tensor(1, self.hidden_dim))\n",
    "        c_t = Variable(torch.Tensor(1, self.hidden_dim))\n",
    "        nn.init.uniform(h_t)\n",
    "        nn.init.uniform(c_t)\n",
    "\n",
    "        # if torch.cuda.is_available():\n",
    "        #     h_t = h_t.cuda()\n",
    "        #     c_t = c_t.cuda()\n",
    "\n",
    "        # forward 1 step\n",
    "        _, h_t, c_t = self.forward_step(feature, h_t, c_t, mode=mode)\n",
    "\n",
    "        # candidates: [score, decoded_sequence, h_t, c_t]\n",
    "        symbol_id = torch.LongTensor([1]).unsqueeze(0)\n",
    "        symbol_id = Variable(symbol_id, volatile=True)\n",
    "        # if torch.cuda.is_available():\n",
    "        #     symbol_id = symbol_id.cuda()\n",
    "        candidates = [[0, symbol_id, h_t, c_t, [get_symbol_id('<s>')]]]\n",
    "\n",
    "        # beam search\n",
    "        t = 0\n",
    "        while t < max_len - 1:\n",
    "            t += 1\n",
    "            tmp_candidates = []\n",
    "            end_flag = True\n",
    "            for score, last_id, h_t, c_t, id_seq in candidates:\n",
    "                if id_seq[-1] == get_symbol_id('</s>'):\n",
    "                    tmp_candidates.append([score, last_id, h_t, c_t, id_seq])\n",
    "                else:\n",
    "                    end_flag = False\n",
    "                    emb = self.B(last_id)\n",
    "                    output, h_t, c_t = self.forward_step(emb, h_t, c_t, mode=mode)\n",
    "                    output = output.squeeze(0).squeeze(0)\n",
    "                    # log softmax\n",
    "                    output = F.log_softmax(output)\n",
    "                    output, indices = torch.sort(output, descending=True)\n",
    "                    output = output[:beam_size]\n",
    "                    indices = indices[:beam_size]\n",
    "                    score_list = score + output\n",
    "                    for score, wid in zip(score_list, indices):\n",
    "                        tmp_candidates.append(\n",
    "                            [score, wid, h_t, c_t, id_seq + [int(wid.data.numpy())]]\n",
    "                        )\n",
    "            if end_flag:\n",
    "                break\n",
    "            # sort by normarized log probs and pick beam_size highest candidate\n",
    "            candidates = sorted(tmp_candidates,\n",
    "                                key=lambda x: -x[0].data.numpy()/len(x[-1]))[:beam_size]\n",
    "\n",
    "        return candidates[0][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from build_vocab import Vocab\n",
    "from data_loader import get_data_loader\n",
    "from data_loader import get_styled_data_loader\n",
    "from models import EncoderCNN\n",
    "from models import FactoredLSTM\n",
    "from loss import masked_cross_entropy\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "\n",
    "def eval_outputs(outputs, vocab):\n",
    "    # outputs: [batch, max_len - 1, vocab_size]\n",
    "    indices = torch.topk(outputs, 1)[1]\n",
    "    indices = indices.squeeze(2)\n",
    "    indices = indices.data\n",
    "    for i in range(len(indices)):\n",
    "        caption = [vocab.i2w[x] for x in indices[i]]\n",
    "        print(caption)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    model_path = args.model_path\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    # load vocablary\n",
    "    with open(args.vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    img_path = args.img_path\n",
    "    factual_cap_path = args.factual_caption_path\n",
    "    humorous_cap_path = args.humorous_caption_path\n",
    "\n",
    "    # import data_loader\n",
    "    data_loader = get_data_loader(img_path, factual_cap_path, vocab,\n",
    "                                  args.caption_batch_size, shuffle=True)\n",
    "    styled_data_loader = get_styled_data_loader(humorous_cap_path, vocab,\n",
    "                                                args.language_batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "    # import models\n",
    "    emb_dim = args.emb_dim\n",
    "    hidden_dim = args.hidden_dim\n",
    "    factored_dim = args.factored_dim\n",
    "    vocab_size = len(vocab)\n",
    "    encoder = EncoderCNN(emb_dim)\n",
    "    decoder = FactoredLSTM(emb_dim, hidden_dim, factored_dim, vocab_size)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        encoder = encoder.cuda()\n",
    "        decoder = decoder.cuda()\n",
    "\n",
    "    # loss and optimizer\n",
    "    criterion = masked_cross_entropy\n",
    "    cap_params = list(decoder.parameters()) + list(encoder.A.parameters())\n",
    "    lang_params = list(decoder.parameters())\n",
    "    optimizer_cap = torch.optim.Adam(cap_params, lr=args.lr_caption)\n",
    "    optimizer_lang = torch.optim.Adam(lang_params, lr=args.lr_language)\n",
    "\n",
    "    # train\n",
    "    total_cap_step = len(data_loader)\n",
    "    total_lang_step = len(styled_data_loader)\n",
    "    epoch_num = args.epoch_num\n",
    "    for epoch in range(epoch_num):\n",
    "        # caption\n",
    "        for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "            images = to_var(images, volatile=True)\n",
    "            captions = to_var(captions.long())\n",
    "\n",
    "            # forward, backward and optimize\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(captions, features, mode=\"factual\")\n",
    "            loss = criterion(outputs[:, 1:, :].contiguous(),\n",
    "                             captions[:, 1:].contiguous(), lengths - 1)\n",
    "            loss.backward()\n",
    "            optimizer_cap.step()\n",
    "\n",
    "            # print log\n",
    "            if i % args.log_step_caption == 0:\n",
    "                print(\"Epoch [%d/%d], CAP, Step [%d/%d], Loss: %.4f\"\n",
    "                      % (epoch+1, epoch_num, i, total_cap_step,\n",
    "                          loss.data.mean()))\n",
    "\n",
    "        eval_outputs(outputs, vocab)\n",
    "\n",
    "        # language\n",
    "        for i, (captions, lengths) in enumerate(styled_data_loader):\n",
    "            captions = to_var(captions.long())\n",
    "\n",
    "            # forward, backward and optimize\n",
    "            decoder.zero_grad()\n",
    "            outputs = decoder(captions, mode='humorous')\n",
    "            loss = criterion(outputs, captions[:, 1:].contiguous(), lengths-1)\n",
    "            loss.backward()\n",
    "            optimizer_lang.step()\n",
    "\n",
    "            # print log\n",
    "            if i % args.log_step_language == 0:\n",
    "                print(\"Epoch [%d/%d], LANG, Step [%d/%d], Loss: %.4f\"\n",
    "                      % (epoch+1, epoch_num, i, total_lang_step,\n",
    "                          loss.data.mean()))\n",
    "\n",
    "        # save models\n",
    "        torch.save(decoder.state_dict(),\n",
    "                   os.path.join(model_path, 'decoder-%d.pkl' % (epoch + 1,)))\n",
    "\n",
    "        torch.save(encoder.state_dict(),\n",
    "                   os.path.join(model_path, 'encoder-%d.pkl' % (epoch + 1,)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "            description='StyleNet: Generating Attractive Visual Captions \\\n",
    "                        with Styles')\n",
    "    parser.add_argument('--model_path', type=str, default='pretrained_models',\n",
    "                        help='path for saving trained models')\n",
    "    parser.add_argument('--vocab_path', type=str, default='./data/vocab.pkl',\n",
    "                        help='path for vocabrary')\n",
    "    parser.add_argument('--img_path', type=str,\n",
    "                        default='./data/flickr7k_images',\n",
    "                        help='path for train images directory')\n",
    "    parser.add_argument('--factual_caption_path', type=str,\n",
    "                        default='./data/factual_train.txt',\n",
    "                        help='path for factual caption file')\n",
    "    parser.add_argument('--humorous_caption_path', type=str,\n",
    "                        default='./data/humor/funny_train.txt',\n",
    "                        help='path for humorous caption file')\n",
    "    parser.add_argument('--romantic_caption_path', type=str,\n",
    "                        default='./data/romantic/romanntic_train.txt',\n",
    "                        help='path for romantic caption file')\n",
    "    parser.add_argument('--caption_batch_size', type=int, default=64,\n",
    "                        help='mini batch size for caption model training')\n",
    "    parser.add_argument('--language_batch_size', type=int, default=96,\n",
    "                        help='mini batch size for language model training')\n",
    "    parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding size of word, image')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=512,\n",
    "                        help='hidden state size of factored LSTM')\n",
    "    parser.add_argument('--factored_dim', type=int, default=512,\n",
    "                        help='size of factored matrix')\n",
    "    parser.add_argument('--lr_caption', type=int, default=0.0002,\n",
    "                        help='learning rate for caption model training')\n",
    "    parser.add_argument('--lr_language', type=int, default=0.0005,\n",
    "                        help='learning rate for language model training')\n",
    "    parser.add_argument('--epoch_num', type=int, default=30)\n",
    "    parser.add_argument('--log_step_caption', type=int, default=50,\n",
    "                        help='steps for print log while train caption model')\n",
    "    parser.add_argument('--log_step_language', type=int, default=10,\n",
    "                        help='steps for print log while train language model')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
