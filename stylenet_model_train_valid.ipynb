{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lERx6Vz1-U9h"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from collections import Counter,defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtWMwnFbAKZw"
   },
   "outputs": [],
   "source": [
    "os.makedirs('.kaggle/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bImuoyNR_ug9"
   },
   "outputs": [],
   "source": [
    "! mv kaggle.json .kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QG-zymPn-bPG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKePlk_OBJEj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chmod: cannot access '/root/.kaggle/kaggle.json': Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "! chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vS1y1XmKBHgT",
    "outputId": "8b007600-bb02-44a5-ca5e-863ef02181bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n",
      "Downloading flicker8k-dataset.zip to /home/ubuntu\n",
      "100%|█████████████████████████████████████▉| 2.07G/2.08G [00:32<00:00, 70.1MB/s]\n",
      "100%|██████████████████████████████████████| 2.08G/2.08G [00:32<00:00, 68.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download ming666/flicker8k-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "mKTPGtpBEPQC",
    "outputId": "6290026e-d9b7-41f2-d2e3-234ee7ebbb83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ubuntu/.kaggle/kaggle.json'\n",
      "Downloading glove6b50dtxt.zip to /home/ubuntu\n",
      " 96%|████████████████████████████████████▌ | 65.0M/67.7M [00:01<00:00, 42.5MB/s]\n",
      "100%|██████████████████████████████████████| 67.7M/67.7M [00:01<00:00, 62.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download watts2/glove6b50dtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iJUpgT3tEclo",
    "outputId": "c403e869-94dc-49ed-e716-aea217930ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove6b50dtxt.zip\n",
      "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "! unzip glove6b50dtxt.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mUQaq92EhUK"
   },
   "outputs": [],
   "source": [
    "! mv glove.6B.50d.txt data/glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OQajMx3gBzpk",
    "outputId": "b1570d3c-fe9c-4ca1-c212-27f97d90e5bd"
   },
   "outputs": [],
   "source": [
    "! unzip flicker8k-dataset.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOrt2NK6CnVT"
   },
   "outputs": [],
   "source": [
    "os.makedirs('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6sB_rejbCwcu"
   },
   "outputs": [],
   "source": [
    "! mv flickr8k_dataset/Flicker8k_Dataset data/Flicker8k_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKK_47RUDJY-"
   },
   "outputs": [],
   "source": [
    "! mv Flickr8k_text data/Flickr8k_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "lq3YD9CJC4sj",
    "outputId": "d14ca3c6-2b8e-4ee7-de18-325594789bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-24 06:43:50--  https://zhegan27.github.io/Papers/FlickrStyle_v0.9.zip\n",
      "Resolving zhegan27.github.io (zhegan27.github.io)... 185.199.109.153, 185.199.110.153, 185.199.111.153, ...\n",
      "Connecting to zhegan27.github.io (zhegan27.github.io)|185.199.109.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 550592 (538K) [application/zip]\n",
      "Saving to: ‘FlickrStyle_v0.9.zip’\n",
      "\n",
      "FlickrStyle_v0.9.zi 100%[===================>] 537.69K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2020-06-24 06:43:50 (44.0 MB/s) - ‘FlickrStyle_v0.9.zip’ saved [550592/550592]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://zhegan27.github.io/Papers/FlickrStyle_v0.9.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "728svaaVDTpK",
    "outputId": "b36413be-2970-4b32-fc3f-e450ad94eb4d"
   },
   "outputs": [],
   "source": [
    "! unzip FlickrStyle_v0.9.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oaBtoGVwDXln"
   },
   "outputs": [],
   "source": [
    "! mv FlickrStyle_v0.9 data/FlickrStyle_v0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2HVCdOU-U9m"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMogiqWl-U9n"
   },
   "source": [
    "### select 7k train images and 1k validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Dt7BjDZ-U9n"
   },
   "outputs": [],
   "source": [
    "img_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BmB48w6NDgZx",
    "outputId": "58e5e962-d53b-4c09-d225-847c8b614424"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WlccOXj-U9t"
   },
   "outputs": [],
   "source": [
    "def select_7k_images(c_type='humor'):\n",
    "    '''8k -> 7k'''\n",
    "    # Create folder if does not exist.\n",
    "    if not os.path.exists('data/Flickr7k/'):\n",
    "        os.makedirs('data/Flickr7k/')\n",
    "    # open data/type/train.p\n",
    "    img_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )\n",
    "    \n",
    "    # copy imgs\n",
    "    for img_name in img_lst:\n",
    "        shutil.copyfile('data/Flicker8k_Dataset/' + img_name,\n",
    "                        'data/Flickr7k/' + img_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7j71iw9-U9v"
   },
   "outputs": [],
   "source": [
    "# Run for first time \n",
    "select_7k_images(c_type='humor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSOrHerq-U9y"
   },
   "outputs": [],
   "source": [
    "def select_validation_images():\n",
    "    '''select test images randomly'''\n",
    "    # get filenames in flickr7k, 30k_images\n",
    "    filenames_7k = os.listdir('data/Flickr7k/')\n",
    "    filenames_8k = os.listdir('data/Flicker8k_Dataset')\n",
    "\n",
    "    filenames = list(set(filenames_8k) - set(filenames_7k))\n",
    "    print(\"img_num: \" + str(len(filenames)))\n",
    "\n",
    "    # copy images\n",
    "    validation_img_path = 'data/validation_images/'\n",
    "    if not os.path.exists(validation_img_path):\n",
    "        os.makedirs(validation_img_path)\n",
    "    for img_name in filenames:\n",
    "        shutil.copyfile('data/Flicker8k_Dataset/' + img_name,\n",
    "                        validation_img_path + img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YmjVhjMd-U90",
    "outputId": "a3c2ae8a-528c-4e09-95b9-ee40a1d3c01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_num: 1091\n"
     ]
    }
   ],
   "source": [
    "# Run for first time\n",
    "select_validation_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sl0g18iT-U93"
   },
   "source": [
    "### select 7k factual captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cEa5itA-U93"
   },
   "outputs": [],
   "source": [
    "flickr8k_filename = \"data/Flickr8k_text/Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQ1vTRTX-U96"
   },
   "outputs": [],
   "source": [
    "img_id_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTWx9u2c-U99"
   },
   "outputs": [],
   "source": [
    "# get filenames in flickr7k_images\n",
    "filenames = os.listdir('data/Flickr7k/')\n",
    "# open factual caption: Flickr8k.token.txt\n",
    "with open(flickr8k_filename, 'r') as f:\n",
    "    res = f.readlines()\n",
    "\n",
    "# write out\n",
    "with open('data/factual_train.txt', 'w') as f:\n",
    "    r = re.compile(r'#\\d*')\n",
    "    for line in res:\n",
    "        img_id = r.split(line)[0]\n",
    "        if img_id in img_id_lst:\n",
    "            f.write(line)\n",
    "            \n",
    "with open('data/factual_valid.txt', 'w') as f:\n",
    "    r = re.compile(r'#\\d*')\n",
    "    for line in res:\n",
    "        img_id = r.split(line)[0]\n",
    "        if img_id not in img_id_lst:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuvQd-Si-U9_"
   },
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAo7iryA-U-A"
   },
   "outputs": [],
   "source": [
    "def extract_captions(mode='factual'):\n",
    "    ''' Extract captions from txt files to build vocabulary.'''\n",
    "    if mode == 'factual':\n",
    "        text = pd.read_csv(\"data/factual_train.txt\", \n",
    "                           encoding= 'unicode_escape', names=['line'])\n",
    "        text['caption'] = text['line'].str.split('\\t', n=1, expand=True)[1]\n",
    "\n",
    "    elif mode == 'humorous':\n",
    "        text = pd.read_csv(\"data/FlickrStyle_v0.9/humor/funny_train.txt\", \n",
    "                           encoding= 'unicode_escape', names=['caption'])\n",
    "    else:\n",
    "        text = pd.read_csv(\"data/FlickrStyle_v0.9/romantic/romantic_train.txt\", \n",
    "                           encoding= 'unicode_escape', names=['caption'])\n",
    "    return list(text.caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWJhU95n-U-K"
   },
   "outputs": [],
   "source": [
    "re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "def sub_br(x): return re_br.sub(\"\\n\", x)\n",
    "\n",
    "my_tok = spacy.load('en')\n",
    "def spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(sub_br(x))]\n",
    "\n",
    "def loadGloveModel(gloveFile=\"data/glove.6B.50d.txt\"):\n",
    "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
    "    f = open(gloveFile,'r')\n",
    "    word_vecs = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
    "    return word_vecs\n",
    "\n",
    "def get_vocab(mode_list=['factual', 'humorous']):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \n",
    "    Delete rare words from vocab if they are not in word_vecs\n",
    "    and don't have at least min_df occurrencies.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for mode in mode_list:\n",
    "        content.extend(extract_captions(mode))\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(spacy_tok(line))\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "def create_embedding_matrix(word_vecs, word_count, min_df=4, emb_size=50):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    #word_count = delete_rare_words(word_vecs, word_count, min_df)\n",
    "    V = len(word_count.keys()) + 2\n",
    "    vocab2index = {}\n",
    "    W = np.zeros((V, emb_size), dtype=\"float32\")\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    # adding a vector for padding\n",
    "    W[0] = np.zeros(emb_size, dtype='float32')\n",
    "    # adding a vector for rare words \n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "    vocab2index[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_count:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "    return W, np.array(vocab), vocab2index\n",
    "\n",
    "def encode_sentence_no_padding(s, vocab2index):\n",
    "    return np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQQ67Abz-U-Q"
   },
   "outputs": [],
   "source": [
    "word_vecs = loadGloveModel()\n",
    "word_count = get_vocab(mode_list=['factual', 'humorous'])\n",
    "# word_count = delete_rare_words(word_vecs, word_count)\n",
    "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs, word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S1IkRAIr-U-T",
    "outputId": "49642ff0-d18f-42b1-c018-dda1ef2494ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10383"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYkiA5fj-U-W"
   },
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plUqNdzV-U-W"
   },
   "outputs": [],
   "source": [
    "def get_img_and_caption(caption_file):\n",
    "    '''\n",
    "    Get image name and caption from factual caption file.\n",
    "    Returns array of tuples.\n",
    "    '''\n",
    "    text = pd.read_csv(caption_file, encoding= 'unicode_escape', names=['line'])\n",
    "    caption = text['line'].str.split('\\t', n=1, expand=True)[1]\n",
    "    img = text['line'].str.split('#', n=1, expand=True)[0]\n",
    "    return list(zip(img, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5g0yK2mF-U-Y"
   },
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    im = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def resize_image(path, sz=112):\n",
    "    im = read_image(path)\n",
    "    return cv2.resize(im, (sz, sz))\n",
    "\n",
    "def normalize(im):\n",
    "    \"\"\"Normalizes images with Imagenet stats.\"\"\"\n",
    "    imagenet_stats = np.array([[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]])\n",
    "    return (im - imagenet_stats[0])/imagenet_stats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0WTZicd-U-a"
   },
   "outputs": [],
   "source": [
    "class Flickr7kDataset(Dataset):\n",
    "    def __init__(self, img_dir, caption_file, vocab2index, transform=None):\n",
    "        '''\n",
    "        img_dir: Directory with all images.\n",
    "        caption_file: Path to factual caption file.\n",
    "        vocab2index: Vocab dictionary.\n",
    "        transform: Optional transforms to apply.\n",
    "        '''\n",
    "        self.img_dir = img_dir\n",
    "        self.img_caption_list = self._get_img_and_caption(caption_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def _get_img_and_caption(self, caption_file):\n",
    "        '''\n",
    "        Get image name and caption from factual caption file.\n",
    "        Returns array of tuples.\n",
    "        '''\n",
    "        text = pd.read_csv(caption_file, encoding= 'unicode_escape', names=['line'])\n",
    "        caption = text['line'].str.split('\\t', n=1, expand=True)[1]\n",
    "        img = text['line'].str.split('#', n=1, expand=True)[0]\n",
    "        return list(zip(img, caption))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_caption_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Returns image and caption embedding.'''\n",
    "        img_name = self.img_caption_list[idx][0]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        if self.transform is not None:\n",
    "#             transform = transforms.Compose([Rescale((224, 224)),transforms.ToTensor()])\n",
    "#             image = transform(image)\n",
    "            image = resize_image(img_path, sz=56)\n",
    "            image = normalize(image)\n",
    "            image = np.rollaxis(image, 2)\n",
    "            \n",
    "        caption = self.img_caption_list[idx][1]\n",
    "        caption = encode_sentence_no_padding(caption, vocab2index)\n",
    "        \n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6y4A6Fg-U-d"
   },
   "outputs": [],
   "source": [
    "img_path = \"data/Flickr7k\"\n",
    "cap_path = \"data/factual_train.txt\"\n",
    "flickr7k_ds = Flickr7kDataset(img_path, cap_path, vocab2index, transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv data/validation_images/3504479370_ff2d89a043.jpg data/test/3504479370_ff2d89a043.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55pshNIO-U-f"
   },
   "outputs": [],
   "source": [
    "image, captions = flickr7k_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kP0GsbFP-U-j",
    "outputId": "e5ab1153-68c8-438c-9f55-63e6badd4ae6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 112, 112)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8u3Jk4y-U-m"
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    '''create minibatch tensors from data(list of tuple(image, caption))'''\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # images\n",
    "    images = [torch.Tensor(im) for im in images]\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # captions\n",
    "    captions = [torch.Tensor(c) for c in captions]\n",
    "    lengths = torch.LongTensor([len(c) for c in captions])\n",
    "    captions_padded = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return images, captions_padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNExjasd-U-o"
   },
   "outputs": [],
   "source": [
    "images, captions, lengths = collate_fn([flickr7k_ds[0], flickr7k_ds[1], flickr7k_ds[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JdfOeqMF-U-q",
    "outputId": "f5ac01a5-aaa2-435d-b299-4ff2493a386b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 112, 112]), torch.Size([18]), tensor(18))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape, captions[0].shape, lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBpBL1fc-U-s"
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "flickr7k_dl = DataLoader(flickr7k_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zP6pwxfP-U-v"
   },
   "outputs": [],
   "source": [
    "valid_img_path = 'data/validation_images'\n",
    "valid_cap_path = \"data/factual_valid.txt\"\n",
    "valid_ds = Flickr7kDataset(valid_img_path, valid_cap_path, vocab2index, transform=True)\n",
    "valid_dl = DataLoader(flickr7k_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "woVPCKXO-U-z"
   },
   "outputs": [],
   "source": [
    "images, captions, lengths = next(iter(flickr7k_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VlxB3t82-U-1",
    "outputId": "ff8f2993-bc07-43a0-d4d5-e90a36cea3cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3, 112, 112]), torch.Size([3, 15]), torch.Size([3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, captions.shape, lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulZ6gRv6-U-4"
   },
   "outputs": [],
   "source": [
    "class FlickrStyle7kDataset(Dataset):\n",
    "    def __init__(self, caption_file, vocab2index):\n",
    "        '''\n",
    "        caption_file: Path to styled caption file.\n",
    "        vocab2index: Vocab dictionary.\n",
    "        '''\n",
    "        self.caption_list = self._get_caption(caption_file)\n",
    "\n",
    "    def _get_caption(self, caption_file):\n",
    "        '''Get list of captions from styled caption file.'''\n",
    "        text = pd.read_csv(caption_file, encoding= 'unicode_escape', \n",
    "                           names=['caption'])\n",
    "        return list(text.caption)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.caption_list[idx]\n",
    "        caption = encode_sentence_no_padding(caption, vocab2index)\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncmAyTRN-U-6"
   },
   "outputs": [],
   "source": [
    "def collate_fn_styled(captions):\n",
    "    # captions\n",
    "    captions = [torch.Tensor(c) for c in captions]\n",
    "    lengths = torch.LongTensor([len(c) for c in captions])\n",
    "    captions_padded = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "\n",
    "    return captions_padded, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9j8dUSUY-U-9"
   },
   "outputs": [],
   "source": [
    "#batch_size \n",
    "#shuffle = True\n",
    "cap_path_styled = \"data/FlickrStyle_v0.9/humor/funny_train.txt\"\n",
    "flickrstyle7k_ds = FlickrStyle7kDataset(cap_path_styled, vocab2index)\n",
    "\n",
    "flickrstyle7k_dl = DataLoader(dataset=flickrstyle7k_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn_styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oe0HytnY-U-_"
   },
   "outputs": [],
   "source": [
    "captions, lengths = next(iter(flickrstyle7k_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pc7gYTRS-U_D",
    "outputId": "544f4a2b-3c8b-4a2b-ecd9-027dae4ed214"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 17]), torch.Size([3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.shape, lengths.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MmRKhOLi-U_H"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Af5noZzZ-U_H"
   },
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    if torch.cuda.is_available():\n",
    "        length = length.cuda()\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "y3uGiPkC-U_J",
    "outputId": "811e817b-8954-4179-971f-85ea5b816266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "         False, False, False]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "length = torch.LongTensor([23, 21, 17])\n",
    "\n",
    "print(sequence_mask(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "On7-iLii-U_L"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwL2oE3c-U_L"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        '''\n",
    "        Load the pretrained ResNet152 and replace fc\n",
    "        '''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.A = nn.Linear(resnet.fc.in_features, emb_dim)\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''Extract the image feature vectors'''\n",
    "        features = self.resnet(images)\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.A(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uY9GsmE-U_N"
   },
   "outputs": [],
   "source": [
    "class FactoredLSTM(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, factored_dim,  vocab_size):\n",
    "        super(FactoredLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # embedding\n",
    "        self.B = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        # factored lstm weights\n",
    "        self.U_i = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fi = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_i = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_i = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_f = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_ff = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_f = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_f = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_o = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fo = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_o = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_c = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fc = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_c = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_c = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # h - humorous\n",
    "        self.S_hi = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_hf = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_ho = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_hc = nn.Linear(factored_dim, factored_dim)\n",
    "\n",
    "        # r - romantic\n",
    "        # self.S_ri = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_rf = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_ro = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_rc = nn.Linear(factored_dim, factored_dim)\n",
    "\n",
    "        # weight for output\n",
    "        self.C = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward_step(self, embedded, h_0, c_0, mode):\n",
    "        # transform embedded from emb_dim --> factored_dim\n",
    "        i = self.V_i(embedded)\n",
    "        f = self.V_f(embedded)\n",
    "        o = self.V_o(embedded)\n",
    "        c = self.V_c(embedded)\n",
    "        \n",
    "        # factored_dim --> factored_dim\n",
    "        if mode == \"factual\":\n",
    "            i = self.S_fi(i)\n",
    "            f = self.S_ff(f)\n",
    "            o = self.S_fo(o)\n",
    "            c = self.S_fc(c)\n",
    "        elif mode == \"humorous\":\n",
    "            i = self.S_hi(i)\n",
    "            f = self.S_hf(f)\n",
    "            o = self.S_ho(o)\n",
    "            c = self.S_hc(c)\n",
    "        # elif mode == \"romantic\":\n",
    "        #     i = self.S_ri(i)\n",
    "        #     f = self.S_rf(f)\n",
    "        #     o = self.S_ro(o)\n",
    "        #     c = self.S_rc(c)\n",
    "        else:\n",
    "            sys.stderr.write(\"mode name wrong!\")\n",
    "\n",
    "        i_t = F.sigmoid(self.U_i(i.double()) + self.W_i(h_0.double()))\n",
    "        f_t = F.sigmoid(self.U_f(f.double()) + self.W_f(h_0.double()))\n",
    "        o_t = F.sigmoid(self.U_o(o.double()) + self.W_o(h_0.double()))\n",
    "        c_tilda = F.tanh(self.U_c(c.double()) + self.W_c(h_0.double()))\n",
    "\n",
    "        c_t = f_t * c_0 + i_t * c_tilda\n",
    "        h_t = o_t * c_t\n",
    "\n",
    "        outputs = self.C(h_t)\n",
    "\n",
    "        return outputs, h_t, c_t\n",
    "\n",
    "    def forward(self, captions, features=None, mode=\"factual\"):\n",
    "        '''\n",
    "        Args:\n",
    "            features: fixed vectors from images, [batch, emb_dim]\n",
    "            captions: [batch, max_len]\n",
    "            mode: type of caption to generate\n",
    "        '''\n",
    "        batch_size = captions.size(0)\n",
    "        embedded = self.B(captions)  # [batch, max_len, emb_dim]\n",
    "        # concat image features and captions\n",
    "        if mode == \"factual\":\n",
    "            if features is None:\n",
    "                sys.stderr.write(\"features is None!\")\n",
    "            embedded = torch.cat((features.unsqueeze(1), embedded), 1)\n",
    "\n",
    "        # initialize hidden state\n",
    "        h_t = torch.Tensor(batch_size, self.hidden_dim)\n",
    "        c_t = torch.Tensor(batch_size, self.hidden_dim)\n",
    "        nn.init.uniform(h_t)\n",
    "        nn.init.uniform(c_t)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h_t = h_t.cuda()\n",
    "            c_t = c_t.cuda()\n",
    "\n",
    "        all_outputs = []\n",
    "        # iterate\n",
    "        for ix in range(embedded.size(1) - 1):\n",
    "            emb = embedded[:, ix, :]\n",
    "            outputs, h_t, c_t = self.forward_step(emb, h_t, c_t, mode=mode)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "        all_outputs = torch.stack(all_outputs, 1)\n",
    "\n",
    "        return all_outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-o7bUHe-U_Q"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDp_Z6Y9-U_Q"
   },
   "outputs": [],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# def eval_outputs(outputs, vocab):\n",
    "#     # outputs: [batch, max_len - 1, vocab_size]\n",
    "#     indices = torch.topk(outputs, 1)[1]\n",
    "#     indices = indices.squeeze(2)\n",
    "#     indices = indices.data\n",
    "#     for i in range(len(indices)):\n",
    "#         caption = [vocab.i2w[x] for x in indices[i]]\n",
    "#         print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05R2rc81-U_T"
   },
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "# data_loader = get_data_loader(img_path, cap_path, vocab, batch_size)\n",
    "# styled_data_loader = get_styled_data_loader(cap_path_styled, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4twHr_C-U_V"
   },
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, vocab2index.items()))\n",
    "reverse_word_map[0]='<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yOlj8Wob-U_X",
    "outputId": "950fed07-be34-481b-9866-bc506b6e8784"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_map[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FsdNCvpS-U_f"
   },
   "outputs": [],
   "source": [
    "def val_metrics(encoder, decoder, valid_fact_dl):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total = 0\n",
    "    step = len(valid_fact_dl)\n",
    "    sum_bleu = 0\n",
    "    for n,(images, captions, lengths) in enumerate(valid_fact_dl):\n",
    "        batch = lengths.shape[0]\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            captions = captions.cuda()\n",
    "        features = encoder(images.double())\n",
    "        outputs = decoder(captions.long(), features.double(), mode=\"factual\")\n",
    "        loss = criterion(outputs[:, 1:, :].contiguous(),\n",
    "                             captions[:, 1:].contiguous().long(), lengths - 1)\n",
    "        #if n% 50 == 0:\n",
    "        print(\"Validation Loss %.3f\" % (loss.data.mean()))\n",
    "        \n",
    "        indices = torch.topk(outputs, 1)[1].squeeze(2).data\n",
    "        for i in range(batch):\n",
    "            predicted_caption = [reverse_word_map[x.item()] for x in indices[i]]\n",
    "            actual_caption = [reverse_word_map[x.item()] for x in captions[i]]\n",
    "            print(predicted_caption, actual_caption)\n",
    "            bleu_score = corpus_bleu(predicted_caption, actual_caption)\n",
    "            print(bleu_score)\n",
    "            sum_bleu += bleu_score\n",
    "            total += 1\n",
    "        if n==3: break\n",
    "    return sum_bleu/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, \n",
    "          train_fact_dl, valid_fact_dl, style_dl, \n",
    "          epoch_num, optimizer_cap, optimizer_lang,\n",
    "          total_cap_step, total_lang_step, model_path):\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        #caption\n",
    "        for i, (images, captions, lengths) in enumerate(train_fact_dl):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                captions = captions.cuda()\n",
    "\n",
    "            # forward, backward and optimize\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            features = encoder(images.double())\n",
    "            outputs = decoder(captions.long(), features.double(), mode=\"factual\")\n",
    "            loss = criterion(outputs[:, 1:, :].contiguous(),\n",
    "                             captions[:, 1:].contiguous().long(), lengths - 1)\n",
    "            loss.backward()\n",
    "            optimizer_cap.step()\n",
    "\n",
    "            #print log\n",
    "            if i % 50 == 0:\n",
    "                print(\"Epoch [%d/%d], CAP, Step [%d/%d], Train Loss: %.4f\"\n",
    "                      % (epoch+1, epoch_num, i, total_cap_step,\n",
    "                          loss.data.mean()))\n",
    "            #if i==2: break\n",
    "            # print(\"Epoch [%d/%d], CAP, Step [%d/%d], Loss: %.4f\"\n",
    "            #           % (epoch+1, epoch_num, i, total_cap_step,\n",
    "            #               loss.data.mean()))\n",
    "\n",
    "        #eval_outputs(outputs, vocab)\n",
    "#         val_bleu = val_metrics(encoder, decoder, valid_fact_dl)\n",
    "#         print(\"val_bleu %.3f\" % (val_bleu))\n",
    "\n",
    "        # language\n",
    "        for i, (captions, lengths) in enumerate(style_dl):\n",
    "            if torch.cuda.is_available():\n",
    "                captions = captions.cuda()\n",
    "\n",
    "            # forward, backward and optimize\n",
    "            decoder.zero_grad()\n",
    "            outputs = decoder(captions.long(), mode='humorous')\n",
    "            loss = criterion(outputs, captions[:, 1:].contiguous().long(), lengths-1)\n",
    "            loss.backward()\n",
    "            optimizer_lang.step()\n",
    "\n",
    "            #print log\n",
    "            if i % 50 == 0:\n",
    "                print(\"Epoch [%d/%d], LANG, Step [%d/%d], Loss: %.4f\"\n",
    "                      % (epoch+1, epoch_num, i, total_lang_step,\n",
    "                          loss.data.mean()))\n",
    "            #if i==3: break\n",
    "            # print(\"Epoch [%d/%d], LANG, Step [%d/%d], Loss: %.4f\"\n",
    "            #           % (epoch+1, epoch_num, i, total_lang_step,\n",
    "            #               loss.data.mean()))\n",
    "\n",
    "        # save models\n",
    "        torch.save(decoder.state_dict(),\n",
    "                   os.path.join(model_path, 'decoder-%d.pkl' % (epoch + 1,)))\n",
    "\n",
    "        torch.save(encoder.state_dict(),\n",
    "                   os.path.join(model_path, 'encoder-%d.pkl' % (epoch + 1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ii285_I-U_m"
   },
   "outputs": [],
   "source": [
    "batch_size_1 = 20\n",
    "batch_size_2 = 40\n",
    "\n",
    "num_workers = 4\n",
    "shuffle = True\n",
    "flickr7k_dl = DataLoader(flickr7k_ds,\n",
    "                         batch_size=batch_size_1,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds,\n",
    "                         batch_size=10,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn)\n",
    "flickrstyle7k_dl = DataLoader(dataset=flickrstyle7k_ds,\n",
    "                         batch_size=batch_size_2,\n",
    "                         shuffle=shuffle,\n",
    "                         num_workers=num_workers,\n",
    "                         collate_fn=collate_fn_styled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POOiQrld-U_o"
   },
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "hidden_dim = 512\n",
    "factored_dim = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "encoder = EncoderCNN(emb_dim)\n",
    "decoder = FactoredLSTM(emb_dim, hidden_dim, factored_dim, vocab_size)\n",
    "\n",
    "encoder = encoder.double()\n",
    "decoder = decoder.double()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dwjUkIJz-U_t"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "total_cap_step = len(flickr7k_dl)\n",
    "total_lang_step = len(flickrstyle7k_dl)\n",
    "epoch_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhzkpGYN-U_v"
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "lr_caption = 0.0002\n",
    "lr_language = 0.0005\n",
    "\n",
    "criterion = masked_cross_entropy\n",
    "cap_params = list(decoder.parameters()) + list(encoder.A.parameters())\n",
    "lang_params = list(decoder.parameters())\n",
    "optimizer_cap = torch.optim.Adam(cap_params, lr=lr_caption)\n",
    "optimizer_lang = torch.optim.Adam(lang_params, lr=lr_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1U9lS2C-U_x"
   },
   "outputs": [],
   "source": [
    "model_path = 'pretrained_models'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "JXSbSKV--U_y",
    "outputId": "e43b0a72-98cf-4884-da34-76bbc06792eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:102: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], CAP, Step [0/1750], Train Loss: 9.1242\n",
      "Epoch [1/5], CAP, Step [50/1750], Train Loss: 5.1986\n",
      "Epoch [1/5], CAP, Step [100/1750], Train Loss: 4.5383\n",
      "Epoch [1/5], CAP, Step [150/1750], Train Loss: 4.6473\n",
      "Epoch [1/5], CAP, Step [200/1750], Train Loss: 4.4983\n",
      "Epoch [1/5], CAP, Step [250/1750], Train Loss: 4.4616\n",
      "Epoch [1/5], CAP, Step [300/1750], Train Loss: 4.2857\n",
      "Epoch [1/5], CAP, Step [350/1750], Train Loss: 4.4231\n",
      "Epoch [1/5], CAP, Step [400/1750], Train Loss: 4.0600\n",
      "Epoch [1/5], CAP, Step [450/1750], Train Loss: 4.0460\n",
      "Epoch [1/5], CAP, Step [500/1750], Train Loss: 3.9267\n",
      "Epoch [1/5], CAP, Step [550/1750], Train Loss: 3.5970\n",
      "Epoch [1/5], CAP, Step [600/1750], Train Loss: 4.1289\n",
      "Epoch [1/5], CAP, Step [650/1750], Train Loss: 3.7435\n",
      "Epoch [1/5], CAP, Step [700/1750], Train Loss: 3.7922\n",
      "Epoch [1/5], CAP, Step [750/1750], Train Loss: 3.8378\n",
      "Epoch [1/5], CAP, Step [800/1750], Train Loss: 4.0018\n",
      "Epoch [1/5], CAP, Step [850/1750], Train Loss: 3.9198\n",
      "Epoch [1/5], CAP, Step [900/1750], Train Loss: 3.9143\n",
      "Epoch [1/5], CAP, Step [950/1750], Train Loss: 3.6421\n",
      "Epoch [1/5], CAP, Step [1000/1750], Train Loss: 3.5797\n",
      "Epoch [1/5], CAP, Step [1050/1750], Train Loss: 3.5653\n",
      "Epoch [1/5], CAP, Step [1100/1750], Train Loss: 3.6468\n",
      "Epoch [1/5], CAP, Step [1150/1750], Train Loss: 3.7839\n",
      "Validation Loss 3.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 7.44 GiB total capacity; 6.84 GiB already allocated; 17.56 MiB free; 7.02 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-6bacc79e2e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m       \u001b[0mflickr7k_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflickrstyle7k_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_cap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       total_cap_step, total_lang_step,model_path)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-99da8db6dd29>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, train_fact_dl, valid_fact_dl, style_dl, epoch_num, optimizer_cap, optimizer_lang, total_cap_step, total_lang_step, model_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#eval_outputs(outputs, vocab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mval_bleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_fact_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val_bleu %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_bleu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-95d6afdc4329>\u001b[0m in \u001b[0;36mval_metrics\u001b[0;34m(encoder, decoder, valid_fact_dl)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"factual\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         loss = criterion(outputs[:, 1:, :].contiguous(),\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-37437f511cc2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m'''Extract the image feature vectors'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1922\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 7.44 GiB total capacity; 6.84 GiB already allocated; 17.56 MiB free; 7.02 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Task 1 - first epoch\n",
    "train(encoder, decoder, \n",
    "      flickr7k_dl, valid_dl, flickrstyle7k_dl, \n",
    "      epoch_num, optimizer_cap, optimizer_lang,\n",
    "      total_cap_step, total_lang_step,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "JXSbSKV--U_y",
    "outputId": "e43b0a72-98cf-4884-da34-76bbc06792eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:102: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], LANG, Step [0/350], Loss: 6.6371\n",
      "Epoch [1/5], LANG, Step [10/350], Loss: 5.5687\n",
      "Epoch [1/5], LANG, Step [20/350], Loss: 5.0289\n",
      "Epoch [1/5], LANG, Step [30/350], Loss: 5.2054\n",
      "Epoch [1/5], LANG, Step [40/350], Loss: 5.2507\n",
      "Epoch [1/5], LANG, Step [50/350], Loss: 4.7173\n",
      "Epoch [1/5], LANG, Step [60/350], Loss: 4.9744\n",
      "Epoch [1/5], LANG, Step [70/350], Loss: 4.3951\n",
      "Epoch [1/5], LANG, Step [80/350], Loss: 4.9487\n",
      "Epoch [1/5], LANG, Step [90/350], Loss: 4.9245\n",
      "Epoch [1/5], LANG, Step [100/350], Loss: 4.7983\n",
      "Epoch [1/5], LANG, Step [110/350], Loss: 4.8542\n",
      "Epoch [1/5], LANG, Step [120/350], Loss: 4.2926\n",
      "Epoch [1/5], LANG, Step [130/350], Loss: 4.9134\n",
      "Epoch [1/5], LANG, Step [140/350], Loss: 4.4444\n",
      "Epoch [1/5], LANG, Step [150/350], Loss: 4.4908\n",
      "Epoch [1/5], LANG, Step [160/350], Loss: 5.0431\n",
      "Epoch [1/5], LANG, Step [170/350], Loss: 4.2022\n",
      "Epoch [1/5], LANG, Step [180/350], Loss: 4.7108\n",
      "Epoch [1/5], LANG, Step [190/350], Loss: 4.8104\n",
      "Epoch [1/5], LANG, Step [200/350], Loss: 4.8454\n",
      "Epoch [1/5], LANG, Step [210/350], Loss: 4.9422\n",
      "Epoch [1/5], LANG, Step [220/350], Loss: 4.3931\n",
      "Epoch [1/5], LANG, Step [230/350], Loss: 4.6030\n",
      "Epoch [1/5], LANG, Step [240/350], Loss: 4.7073\n",
      "Epoch [1/5], LANG, Step [250/350], Loss: 4.5551\n",
      "Epoch [1/5], LANG, Step [260/350], Loss: 4.2341\n",
      "Epoch [1/5], LANG, Step [270/350], Loss: 5.1859\n",
      "Epoch [1/5], LANG, Step [280/350], Loss: 4.5791\n",
      "Epoch [1/5], LANG, Step [290/350], Loss: 4.2746\n",
      "Epoch [1/5], LANG, Step [300/350], Loss: 4.2531\n",
      "Epoch [1/5], LANG, Step [310/350], Loss: 4.5386\n",
      "Epoch [1/5], LANG, Step [320/350], Loss: 4.4750\n",
      "Epoch [1/5], LANG, Step [330/350], Loss: 4.2839\n",
      "Epoch [1/5], LANG, Step [340/350], Loss: 4.2480\n",
      "Epoch [2/5], LANG, Step [0/350], Loss: 3.8203\n",
      "Epoch [2/5], LANG, Step [10/350], Loss: 4.4770\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-6bacc79e2e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m       \u001b[0mflickr7k_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflickrstyle7k_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_cap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       total_cap_step, total_lang_step,model_path)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-b58e5660af3a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, train_fact_dl, valid_fact_dl, style_dl, epoch_num, optimizer_cap, optimizer_lang, total_cap_step, total_lang_step, model_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# forward, backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Task 2 - first epoch\n",
    "train(encoder, decoder, \n",
    "      flickr7k_dl, valid_dl, flickrstyle7k_dl, \n",
    "      epoch_num, optimizer_cap, optimizer_lang,\n",
    "      total_cap_step, total_lang_step,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "JXSbSKV--U_y",
    "outputId": "e43b0a72-98cf-4884-da34-76bbc06792eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:102: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], CAP, Step [0/1750], Train Loss: 3.9546\n",
      "Epoch [1/5], CAP, Step [50/1750], Train Loss: 3.1573\n",
      "Epoch [1/5], CAP, Step [100/1750], Train Loss: 3.7334\n",
      "Epoch [1/5], CAP, Step [150/1750], Train Loss: 3.3264\n",
      "Epoch [1/5], CAP, Step [200/1750], Train Loss: 3.4457\n",
      "Epoch [1/5], CAP, Step [250/1750], Train Loss: 3.3678\n",
      "Epoch [1/5], CAP, Step [300/1750], Train Loss: 3.4562\n",
      "Epoch [1/5], CAP, Step [350/1750], Train Loss: 3.3256\n",
      "Epoch [1/5], CAP, Step [400/1750], Train Loss: 3.9985\n",
      "Epoch [1/5], CAP, Step [450/1750], Train Loss: 3.2154\n",
      "Epoch [1/5], CAP, Step [500/1750], Train Loss: 3.5057\n",
      "Epoch [1/5], CAP, Step [550/1750], Train Loss: 3.1751\n",
      "Epoch [1/5], CAP, Step [600/1750], Train Loss: 3.1273\n",
      "Epoch [1/5], CAP, Step [650/1750], Train Loss: 3.2453\n",
      "Epoch [1/5], CAP, Step [700/1750], Train Loss: 3.3597\n",
      "Epoch [1/5], CAP, Step [750/1750], Train Loss: 3.0249\n",
      "Epoch [1/5], CAP, Step [800/1750], Train Loss: 3.8213\n",
      "Epoch [1/5], CAP, Step [850/1750], Train Loss: 3.4782\n",
      "Epoch [1/5], CAP, Step [900/1750], Train Loss: 3.2368\n",
      "Epoch [1/5], CAP, Step [950/1750], Train Loss: 3.8039\n",
      "Epoch [1/5], CAP, Step [1000/1750], Train Loss: 3.3042\n",
      "Epoch [1/5], CAP, Step [1050/1750], Train Loss: 3.0703\n",
      "Epoch [1/5], CAP, Step [1100/1750], Train Loss: 3.5205\n",
      "Epoch [1/5], CAP, Step [1150/1750], Train Loss: 3.3101\n",
      "Epoch [1/5], CAP, Step [1200/1750], Train Loss: 3.8864\n",
      "Epoch [1/5], CAP, Step [1250/1750], Train Loss: 3.2403\n",
      "Epoch [1/5], CAP, Step [1300/1750], Train Loss: 3.1684\n",
      "Epoch [1/5], CAP, Step [1350/1750], Train Loss: 3.5017\n",
      "Epoch [1/5], CAP, Step [1400/1750], Train Loss: 2.8770\n",
      "Epoch [1/5], CAP, Step [1450/1750], Train Loss: 3.7146\n",
      "Epoch [1/5], CAP, Step [1500/1750], Train Loss: 3.4900\n",
      "Epoch [1/5], CAP, Step [1550/1750], Train Loss: 3.0639\n",
      "Epoch [1/5], CAP, Step [1600/1750], Train Loss: 3.7195\n",
      "Epoch [1/5], CAP, Step [1650/1750], Train Loss: 3.1628\n",
      "Epoch [1/5], CAP, Step [1700/1750], Train Loss: 3.2118\n",
      "Epoch [1/5], LANG, Step [0/350], Loss: 4.2360\n",
      "Epoch [1/5], LANG, Step [50/350], Loss: 4.0825\n",
      "Epoch [1/5], LANG, Step [100/350], Loss: 4.2765\n",
      "Epoch [1/5], LANG, Step [150/350], Loss: 4.1087\n",
      "Epoch [1/5], LANG, Step [200/350], Loss: 4.0694\n",
      "Epoch [1/5], LANG, Step [250/350], Loss: 4.1401\n",
      "Epoch [1/5], LANG, Step [300/350], Loss: 3.9202\n",
      "Epoch [2/5], CAP, Step [0/1750], Train Loss: 3.2129\n",
      "Epoch [2/5], CAP, Step [50/1750], Train Loss: 3.1654\n",
      "Epoch [2/5], CAP, Step [100/1750], Train Loss: 3.5644\n",
      "Epoch [2/5], CAP, Step [150/1750], Train Loss: 3.0134\n",
      "Epoch [2/5], CAP, Step [200/1750], Train Loss: 3.1541\n",
      "Epoch [2/5], CAP, Step [250/1750], Train Loss: 3.0982\n",
      "Epoch [2/5], CAP, Step [300/1750], Train Loss: 3.3143\n",
      "Epoch [2/5], CAP, Step [350/1750], Train Loss: 3.1252\n",
      "Epoch [2/5], CAP, Step [400/1750], Train Loss: 2.9025\n",
      "Epoch [2/5], CAP, Step [450/1750], Train Loss: 3.0346\n",
      "Epoch [2/5], CAP, Step [500/1750], Train Loss: 2.8994\n",
      "Epoch [2/5], CAP, Step [550/1750], Train Loss: 2.6525\n",
      "Epoch [2/5], CAP, Step [600/1750], Train Loss: 3.2392\n",
      "Epoch [2/5], CAP, Step [650/1750], Train Loss: 3.1752\n",
      "Epoch [2/5], CAP, Step [700/1750], Train Loss: 2.7626\n",
      "Epoch [2/5], CAP, Step [750/1750], Train Loss: 3.2145\n",
      "Epoch [2/5], CAP, Step [800/1750], Train Loss: 3.3882\n",
      "Epoch [2/5], CAP, Step [850/1750], Train Loss: 2.9195\n",
      "Epoch [2/5], CAP, Step [900/1750], Train Loss: 2.9501\n",
      "Epoch [2/5], CAP, Step [950/1750], Train Loss: 2.8301\n",
      "Epoch [2/5], CAP, Step [1000/1750], Train Loss: 2.8526\n",
      "Epoch [2/5], CAP, Step [1050/1750], Train Loss: 3.1393\n",
      "Epoch [2/5], CAP, Step [1100/1750], Train Loss: 3.7666\n",
      "Epoch [2/5], CAP, Step [1150/1750], Train Loss: 3.0028\n",
      "Epoch [2/5], CAP, Step [1200/1750], Train Loss: 3.3150\n",
      "Epoch [2/5], CAP, Step [1250/1750], Train Loss: 3.5968\n",
      "Epoch [2/5], CAP, Step [1300/1750], Train Loss: 3.3171\n",
      "Epoch [2/5], CAP, Step [1350/1750], Train Loss: 2.8448\n",
      "Epoch [2/5], CAP, Step [1400/1750], Train Loss: 3.4868\n",
      "Epoch [2/5], CAP, Step [1450/1750], Train Loss: 2.7899\n",
      "Epoch [2/5], CAP, Step [1500/1750], Train Loss: 3.0781\n",
      "Epoch [2/5], CAP, Step [1550/1750], Train Loss: 3.2037\n",
      "Epoch [2/5], CAP, Step [1600/1750], Train Loss: 3.2013\n",
      "Epoch [2/5], CAP, Step [1650/1750], Train Loss: 3.1083\n",
      "Epoch [2/5], CAP, Step [1700/1750], Train Loss: 3.1253\n",
      "Epoch [2/5], LANG, Step [0/350], Loss: 3.4893\n",
      "Epoch [2/5], LANG, Step [50/350], Loss: 3.2558\n",
      "Epoch [2/5], LANG, Step [100/350], Loss: 3.5996\n",
      "Epoch [2/5], LANG, Step [150/350], Loss: 3.5637\n",
      "Epoch [2/5], LANG, Step [200/350], Loss: 3.6542\n",
      "Epoch [2/5], LANG, Step [250/350], Loss: 3.6892\n",
      "Epoch [2/5], LANG, Step [300/350], Loss: 3.6414\n",
      "Epoch [3/5], CAP, Step [0/1750], Train Loss: 3.1633\n",
      "Epoch [3/5], CAP, Step [50/1750], Train Loss: 3.0717\n",
      "Epoch [3/5], CAP, Step [100/1750], Train Loss: 2.7831\n",
      "Epoch [3/5], CAP, Step [150/1750], Train Loss: 3.1288\n",
      "Epoch [3/5], CAP, Step [200/1750], Train Loss: 2.7056\n",
      "Epoch [3/5], CAP, Step [250/1750], Train Loss: 2.8454\n",
      "Epoch [3/5], CAP, Step [300/1750], Train Loss: 2.7349\n",
      "Epoch [3/5], CAP, Step [350/1750], Train Loss: 3.0080\n",
      "Epoch [3/5], CAP, Step [400/1750], Train Loss: 3.3531\n",
      "Epoch [3/5], CAP, Step [450/1750], Train Loss: 2.6312\n",
      "Epoch [3/5], CAP, Step [500/1750], Train Loss: 2.9664\n",
      "Epoch [3/5], CAP, Step [550/1750], Train Loss: 3.4063\n",
      "Epoch [3/5], CAP, Step [600/1750], Train Loss: 2.7075\n",
      "Epoch [3/5], CAP, Step [650/1750], Train Loss: 2.7614\n",
      "Epoch [3/5], CAP, Step [700/1750], Train Loss: 2.9322\n",
      "Epoch [3/5], CAP, Step [750/1750], Train Loss: 2.7749\n",
      "Epoch [3/5], CAP, Step [800/1750], Train Loss: 3.1450\n",
      "Epoch [3/5], CAP, Step [850/1750], Train Loss: 2.9879\n",
      "Epoch [3/5], CAP, Step [900/1750], Train Loss: 2.7018\n",
      "Epoch [3/5], CAP, Step [950/1750], Train Loss: 2.6266\n",
      "Epoch [3/5], CAP, Step [1000/1750], Train Loss: 3.2152\n",
      "Epoch [3/5], CAP, Step [1050/1750], Train Loss: 2.3858\n",
      "Epoch [3/5], CAP, Step [1100/1750], Train Loss: 3.1570\n",
      "Epoch [3/5], CAP, Step [1150/1750], Train Loss: 2.8085\n",
      "Epoch [3/5], CAP, Step [1200/1750], Train Loss: 2.6688\n",
      "Epoch [3/5], CAP, Step [1250/1750], Train Loss: 3.0176\n",
      "Epoch [3/5], CAP, Step [1300/1750], Train Loss: 2.9631\n",
      "Epoch [3/5], CAP, Step [1350/1750], Train Loss: 2.8830\n",
      "Epoch [3/5], CAP, Step [1400/1750], Train Loss: 2.8704\n",
      "Epoch [3/5], CAP, Step [1450/1750], Train Loss: 2.9104\n",
      "Epoch [3/5], CAP, Step [1500/1750], Train Loss: 3.1071\n",
      "Epoch [3/5], CAP, Step [1550/1750], Train Loss: 2.7889\n",
      "Epoch [3/5], CAP, Step [1600/1750], Train Loss: 2.6282\n",
      "Epoch [3/5], CAP, Step [1650/1750], Train Loss: 2.9077\n",
      "Epoch [3/5], CAP, Step [1700/1750], Train Loss: 2.7574\n",
      "Epoch [3/5], LANG, Step [0/350], Loss: 2.9908\n",
      "Epoch [3/5], LANG, Step [50/350], Loss: 3.0973\n",
      "Epoch [3/5], LANG, Step [100/350], Loss: 3.1029\n",
      "Epoch [3/5], LANG, Step [150/350], Loss: 3.1971\n",
      "Epoch [3/5], LANG, Step [200/350], Loss: 2.9978\n",
      "Epoch [3/5], LANG, Step [250/350], Loss: 3.0137\n",
      "Epoch [3/5], LANG, Step [300/350], Loss: 3.4519\n",
      "Epoch [4/5], CAP, Step [0/1750], Train Loss: 2.7166\n",
      "Epoch [4/5], CAP, Step [50/1750], Train Loss: 2.6583\n",
      "Epoch [4/5], CAP, Step [100/1750], Train Loss: 2.8363\n",
      "Epoch [4/5], CAP, Step [150/1750], Train Loss: 2.3192\n",
      "Epoch [4/5], CAP, Step [200/1750], Train Loss: 2.6775\n",
      "Epoch [4/5], CAP, Step [250/1750], Train Loss: 2.8439\n",
      "Epoch [4/5], CAP, Step [300/1750], Train Loss: 3.0510\n",
      "Epoch [4/5], CAP, Step [350/1750], Train Loss: 2.8000\n",
      "Epoch [4/5], CAP, Step [400/1750], Train Loss: 2.6017\n",
      "Epoch [4/5], CAP, Step [450/1750], Train Loss: 2.6520\n",
      "Epoch [4/5], CAP, Step [500/1750], Train Loss: 2.5849\n",
      "Epoch [4/5], CAP, Step [550/1750], Train Loss: 2.7134\n",
      "Epoch [4/5], CAP, Step [600/1750], Train Loss: 2.8541\n",
      "Epoch [4/5], CAP, Step [650/1750], Train Loss: 2.7291\n",
      "Epoch [4/5], CAP, Step [700/1750], Train Loss: 2.4564\n",
      "Epoch [4/5], CAP, Step [750/1750], Train Loss: 2.8174\n",
      "Epoch [4/5], CAP, Step [800/1750], Train Loss: 2.6001\n",
      "Epoch [4/5], CAP, Step [850/1750], Train Loss: 2.7544\n",
      "Epoch [4/5], CAP, Step [900/1750], Train Loss: 2.7163\n",
      "Epoch [4/5], CAP, Step [950/1750], Train Loss: 2.6980\n",
      "Epoch [4/5], CAP, Step [1000/1750], Train Loss: 2.7423\n",
      "Epoch [4/5], CAP, Step [1050/1750], Train Loss: 2.5710\n",
      "Epoch [4/5], CAP, Step [1100/1750], Train Loss: 2.4575\n",
      "Epoch [4/5], CAP, Step [1150/1750], Train Loss: 2.5187\n",
      "Epoch [4/5], CAP, Step [1200/1750], Train Loss: 2.5209\n",
      "Epoch [4/5], CAP, Step [1250/1750], Train Loss: 2.6299\n",
      "Epoch [4/5], CAP, Step [1300/1750], Train Loss: 2.5054\n",
      "Epoch [4/5], CAP, Step [1350/1750], Train Loss: 2.7548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], CAP, Step [1400/1750], Train Loss: 2.4476\n",
      "Epoch [4/5], CAP, Step [1450/1750], Train Loss: 3.0776\n",
      "Epoch [4/5], CAP, Step [1500/1750], Train Loss: 2.8273\n",
      "Epoch [4/5], CAP, Step [1550/1750], Train Loss: 2.6042\n",
      "Epoch [4/5], CAP, Step [1600/1750], Train Loss: 3.0436\n",
      "Epoch [4/5], CAP, Step [1650/1750], Train Loss: 2.5882\n",
      "Epoch [4/5], CAP, Step [1700/1750], Train Loss: 2.5414\n",
      "Epoch [4/5], LANG, Step [0/350], Loss: 2.5110\n",
      "Epoch [4/5], LANG, Step [50/350], Loss: 2.3639\n",
      "Epoch [4/5], LANG, Step [100/350], Loss: 2.4572\n",
      "Epoch [4/5], LANG, Step [150/350], Loss: 2.6094\n",
      "Epoch [4/5], LANG, Step [200/350], Loss: 2.6223\n",
      "Epoch [4/5], LANG, Step [250/350], Loss: 3.0079\n",
      "Epoch [4/5], LANG, Step [300/350], Loss: 2.5911\n",
      "Epoch [5/5], CAP, Step [0/1750], Train Loss: 2.8887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-6bacc79e2e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m       \u001b[0mflickr7k_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflickrstyle7k_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0mepoch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_cap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m       total_cap_step, total_lang_step,model_path)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-fd62743ead76>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, train_fact_dl, valid_fact_dl, style_dl, epoch_num, optimizer_cap, optimizer_lang, total_cap_step, total_lang_step, model_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"factual\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             loss = criterion(outputs[:, 1:, :].contiguous(),\n\u001b[1;32m     19\u001b[0m                              captions[:, 1:].contiguous().long(), lengths - 1)\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-159316eeb74e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, captions, features, mode)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mh_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mc_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Task 1 + Task 2 : remaining 4 epochs\n",
    "train(encoder, decoder, \n",
    "      flickr7k_dl, valid_dl, flickrstyle7k_dl, \n",
    "      epoch_num, optimizer_cap, optimizer_lang,\n",
    "      total_cap_step, total_lang_step,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:102: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss 2.969\n",
      "['dog', 'are', 'a', 'a', 'man', 'in', 'down', 'a', 'snowy', '.', 'trail', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['People', 'watch', 'as', 'a', 'person', 'skis', 'down', 'a', 'mountain', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0931616654031189e-231\n",
      "['dog', 'black', 'dog', 'dog', 'white', 'dog', 'is', 'in', 'the', 'water', '.', 'a', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'spotted', 'black', 'and', 'white', 'dog', 'splashes', 'in', 'the', 'water', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2097822504111573e-231\n",
      "['black', 'old', 'in', 'is', 'on', 'a', 'of', 'a', 'art', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['An', 'unshaven', 'man', 'sits', 'in', 'front', 'of', 'an', 'evergreen', 'tree', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.811219827315784e-232\n",
      "['the', 'man', 'does', 'in', 'the', 'air', 'above', 'a', 'indoor', 'ramp', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'skateboarder', 'high', 'in', 'the', 'air', 'above', 'an', 'indoor', 'ramp', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.26368079958909e-231\n",
      "['UNK', 'dog', 'in', 'a', 'white', 'suit', 'and', 'standing', 'next', 'the', 'doorway', '.', 'area', 'a', 'area', \"'s\", 'been', 'cigarette', 'of', 'its', '.', 'the', 'background', '.'] ['a', 'man', 'in', 'a', 'stripy', 'hat', 'is', 'standing', 'in', 'a', 'house', 'built', 'from', 'ice', 'that', 'has', 'a', 'pair', 'of', 'boots', 'in', 'the', 'doorway', '.']\n",
      "1.4693090644643686e-231\n",
      "['little', 'little', 'in', 'a', 'orange', 'shirt', 'is', 'playing', 'with', 'a', 'young', 'in', 'a', 'black', 'shirt', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'woman', 'in', 'an', 'orange', 'dress', 'is', 'dancing', 'with', 'a', 'man', 'in', 'a', 'black', 'suit', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.4347128449946335e-231\n",
      "['man', 'little', 'in', 'a', 'skateboard', 'and', 'bike', 'in', 'down', 'a', 'woods', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'rider'] ['A', 'man', 'on', 'a', 'red', 'dirt', 'bike', 'drives', 'in', 'the', 'mud', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1023036058264496e-231\n",
      "['people', 'skateboarder', 'men', 'are', 'at', 'front', 'air', 'while', 'front', 'urban', 'match', 'catch', 'the', 'race', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['The', 'three', 'men', 'leaped', 'in', 'the', 'air', 'in', 'an', 'effort', 'to', 'grab', 'the', 'ball', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.281322106854149e-231\n",
      "['man', 'man', 'climbers', 'is', 'to', 'the', 'a', 'photo', 'opportunity', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['The', 'mountain', 'climber', 'prepares', 'to', 'take', 'a', 'photo', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1790700549142423e-231\n",
      "['the', 'man', 'sits', 'a', 'bridge', 'over', 'at', 'at', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['a', 'family', 'crossing', 'a', 'bridge', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0088115117427719e-231\n",
      "['people', 'in', 'red', 'holding', 'and', 'on', 'a', 'crowd', '.', 'a', 'white', 'of', 'in', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Woman', 'in', 'multicolor', 'skirt', 'airborne', 'above', 'a', 'bed', 'with', 'a', 'multicolor', 'bedspread', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0473875838281341e-231\n",
      "['black', 'group', 'in', 'as', 'a', 'and', 'up', 'the', 'air', '.', 'lights', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'biker', 'dressed', 'in', 'white', 'rides', 'through', 'the', 'forest', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.418382295637229e-232\n",
      "['people', 'group', 'man', 'is', 'a', 'helmet', 'hair', 'and', 'a', 'helmet', 'helmet', 'is', 'riding', 'towards', 'a', 'busy', '.', 'area', '.', '.', '.', '.', '.', '.'] ['A', 'young', 'man', 'with', 'gold', 'short', 'shorts', 'and', 'a', 'black', 'top', 'is', 'walking', 'in', 'a', 'parade', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.295028036139838e-231\n",
      "['dog', 'dogs', 'up', 'are', 'playing', 'in', 'in', 'a', 'field', 'field', '.', 'a', 'of', 'a', 'white', '.', 'their', '.', '.', '.', '.', '.', '.', '.'] ['Similarly', 'dressed', 'children', 'are', 'playing', 'soccer', 'in', 'a', 'rocky', 'field', 'in', 'front', 'of', 'a', 'building', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2522435979686076e-231\n",
      "['dog', 'black', 'dog', 'is', 'running', 'the', 'field', 'field', '.', 'in', 'to', 'a', 'a', 'the', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['a', 'brown', 'dog', 'is', 'in', 'a', 'grassy', 'field', 'twisting', 'itself', 'around', 'to', 'see', 'underneath', 'itself', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1826351405327318e-231\n",
      "['dog', 'black', 'golden', 'man', 'is', 'the', 'stick', 'in', 'the', 'lake', 'beach', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['The', 'scruffy', 'young', 'man', 'smokes', 'a', 'cigarette', 'on', 'a', 'busy', 'street', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0931616654031189e-231\n",
      "['woman', 'woman', 'in', 'holding', 'a', 'and', 'large', 'to', 'a', 'right', '.', 'a', 'people', 'watch', 'on', 'a', 'table', '.', 'them', '.', '.', '.', '.', '.'] ['A', 'child', 'is', 'eating', 'something', 'very', 'close', 'to', 'the', 'camera', 'while', 'other', 'children', 'sit', 'at', 'a', 'table', 'behind', 'him', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.227193663866655e-231\n",
      "['on', 'people', 'dogs', 'are', 'racing', 'through', 'snow', 'obstacle', 'field', 'area', '.', 'mountains', 'in', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'black', 'dogs', 'are', 'trotting', 'through', 'an', 'empty', 'public', 'area', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2853701807846192e-231\n",
      "['little', 'little', 'is', 'a', 'are', 'playing', 'in', 'a', 'puddle', 'pool', '.', 'a', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'man', 'and', 'dog', 'are', 'splashing', 'in', 'a', 'swimming', 'pool', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1437444852184351e-231\n",
      "['of', 'white', 'is', 'through', 'the', 'field', 'of', 'a', 'in', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'dog', 'walks', 'through', 'a', 'field', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0518351895246305e-231\n",
      "Validation Loss 2.677\n",
      "['white', 'little', 'boy', 'is', 'in', 'head', '.', 'a', 'side', 'rope', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['a', 'little', 'boy', 'spilled', 'his', 'milk', 'from', 'the', 'green', 'cup', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0939416599524108e-231\n",
      "['girl', 'girl', 'in', 'a', 'wetsuit', 'shirt', 'paddles', 'a', 'water', 'with', 'his', 'around', 'around', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'man', 'in', 'a', 'red', 'kayak', 'in', 'the', 'water', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0216652050829071e-231\n",
      "['the', 'group', 'of', 'a', 'man', 'in', 'a', 'rope', 'on', 'in', 'a', 'at', 'the', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'closeup', 'of', 'a', 'girl', 'holding', 'a', 'heart', 'necklace', 'and', 'looking', 'at', 'it', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1818238379076862e-231\n",
      "['dog', 'dogs', 'are', 'on', 'a', 'edge', 'at', 'at', 'the', 'water', '.', 'embrace', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'people', 'stand', 'on', 'the', 'pier', 'looking', 'at', 'the', 'ocean', 'and', 'embrace', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2153055438588388e-231\n",
      "['white', 'man', 'in', 'in', 'with', 'as', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'boy', 'plays', 'baseball', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "0\n",
      "['black', 'black', 'man', 'is', 'a', 'white', 'shirt', 'and', 'sunglasses', 'hat', 'sunglasses', 'cap', 'is', 'on', 'a', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'young', 'man', 'in', 'a', 'white', 'shirt', 'and', 'gold', 'and', 'black', 'hat', 'sits', 'cross', 'legged', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.3568676344828118e-231\n",
      "['black', 'dogs', 'are', 'in', 'the', 'of', 'a', 'building', 'looking', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'dogs', 'wrestling', 'in', 'front', 'of', 'a', 'fireplace', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0881585058887664e-231\n",
      "['man', 'dogs', 'sized', 'dogs', 'brown', 'dogs', 'are', 'fighting', 'in', 'of', 'war', 'with', 'a', 'green', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'medium', 'sized', 'light', 'brown', 'dogs', 'are', 'playing', 'tug', 'of', 'war', 'with', 'a', 'dish', 'cloth', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.3949832735659357e-231\n",
      "['people', 'group', 'woman', 'in', 'a', 'eyes', 'and', 'with', 'a', 'tire', 'couch', 'white', 'floor', 'toy', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'young', 'boy', 'with', 'blue', 'eyes', 'plays', 'on', 'a', 'red', 'and', 'yellow', 'playground', 'set', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2183324802375697e-231\n",
      "['dog', 'woman', 'is', 'in', 'the', 'ring', 'on', 'water', '.', 'bushes', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'dog', 'jumping', 'through', 'a', 'ring', 'of', 'fire', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0244914152188952e-231\n",
      "['woman', 'man', 'in', 'a', 'hair', 'sunglasses', 'sunglasses', 'sunglasses', 'and', 'the', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['a', 'man', 'with', 'dreeds', 'wearing', 'large', 'black', 'sunglasses', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1883617097442433e-231\n",
      "['dog', 'black', 'is', 'running', 'through', 'the', 'grassy', '.', 'a', 'yellow', 'in', 'his', 'mouth', '.', 'it', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'dog', 'is', 'running', 'through', 'a', 'creek', 'with', 'a', 'stick', 'in', 'its', 'mouth', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.3153652640281735e-231\n",
      "['man', 'in', 'at', 'two', 'watch', 'pictures', 'of', 'snow', 'in', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Woman', 'pointing', 'while', 'others', 'take', 'pictures', 'of', 'something', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1872004050154218e-231\n",
      "['man', 'dogs', 'are', 'through', 'the', 'snow', 'with', 'a', 'stick', 'in', 'the', 'mouths', '.', 'up', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'dogs', 'run', 'through', 'the', 'water', 'with', 'a', 'rope', 'in', 'their', 'mouths', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.3771755356041373e-231\n",
      "['man', 'man', 'in', 'a', 'red', 'backpack', 'and', 'a', 'jacket', 'is', 'a', 'mask', 'mountains', 'in', 'the', 'background', '.', '.', '.', '.', '.', '.', '.'] ['a', 'man', 'wearing', 'a', 'huge', 'backpack', 'and', 'blue', 'jacket', 'with', 'many', 'distant', 'mountains', 'in', 'the', 'background', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.4850004965416568e-231\n",
      "['black', 'group', 'in', 'a', 'woman', 'are', 'for', 'the', 'camera', '.', 'a', 'in', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'man', 'and', 'a', 'woman', 'smile', 'for', 'the', 'camera', 'while', 'standing', 'outside', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2446110438861166e-231\n",
      "['football', 'football', 'woman', 'is', 'jumping', 'the', 'on', 'a', '.', 'a', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'redheaded', 'child', 'is', 'happily', 'swinging', 'in', 'park', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.482529355042618e-232\n",
      "['young', 'men', 'boys', 'are', 'a', 'dad', 'are', '.', 'a', 'floor', 'of', 'a', 'white', '.', 'their', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'little', 'boys', 'and', 'their', 'mom', 'smiling', 'on', 'the', 'outside', 'of', 'a', 'window', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1200407237786664e-231\n",
      "['dog', 'brown', 'is', 'jumping', 'in', 'the', 'snow', 'in', 'front', 'with', 'a', 'stick', 'in', 'his', 'mouth', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'dog', 'is', 'standing', 'in', 'the', 'fenced', 'in', 'field', 'with', 'a', 'stick', 'in', 'his', 'mouth', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.4570125101586512e-231\n",
      "['black', 'man', 'in', 'a', 'woman', 'are', 'standing', 'on', 'a', 'balcony', 'bench', 'next', 'graffiti', 'people', 'around', 'down', 'the', 'street', '.', 'has', 'down', 'him', '.'] ['A', 'man', 'and', 'a', 'dog', 'are', 'sitting', 'on', 'a', 'park', 'bench', 'with', 'several', 'people', 'walking', 'down', 'the', 'path', 'that', 'runs', 'alongside', 'it', '.']\n",
      "1.5490916516112083e-231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss 2.897\n",
      "['black', 'people', 'are', 'standing', 'at', 'sitting', 'are', 'listening', 'around', 'to', 'each', 'brick', 'wall', '.', 'a', 'art', 'gallery', '.'] ['some', 'people', 'are', 'sitting', 'and', 'others', 'are', 'standing', 'next', 'to', 'a', 'false', 'wall', 'in', 'an', 'art', 'gallery', '.']\n",
      "1.5881433504932496e-231\n",
      "['the', 'man', 'is', 'climbing', 'a', 'wave', 'bike', 'through', 'a', 'trail', '.', 'down', '.', '.', '.', '.', '.', '.'] ['This', 'person', 'is', 'riding', 'a', 'BMX', 'bike', 'on', 'a', 'clifftop', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2837951622885606e-231\n",
      "['dog', 'and', 'with', 'mouth', 'collar', 'in', 'in', 'in', 'snow', 'leaves', '.', 'low', '.', '.', '.', '.', '.', '.'] ['Black', 'dog', 'with', 'orange', 'ball', 'approaches', 'camera', 'across', 'dead', 'leaves', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1441414120472538e-231\n",
      "['woman', 'man', 'girl', 'is', 'off', 'rail', 'down', 'a', 'ramp', 'ramp', '.', 'graffiti', 'graffiti', '.', 'graffiti', '.', '.', '.'] ['A', 'young', 'man', 'jumps', 'a', 'bicycle', 'off', 'a', 'sloped', 'wall', 'over', 'a', 'bench', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1323733057643588e-231\n",
      "['dog', 'man', 'is', 'the', 'beach', 'is', 'a', 'a', 'stick', 'stick', 'its', 'in', 'its', '.', '.', '.', '.', '.'] ['The', 'dog', 'on', 'the', 'beach', 'has', 'gotten', 'a', 'hold', 'of', 'something', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1875749753113244e-231\n",
      "['woman', 'and', 'jumping', 'on', 'a', 'with', 'green', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Brown', 'dog', 'runs', 'down', 'ramp', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0518351895246305e-231\n",
      "['woman', 'woman', 'in', 'a', 'asleep', 'in', 'a', 'hands', 'open', 'and', 'wide', '.', '.', '.', '.', '.', '.', '.'] ['A', 'man', 'has', 'fallen', 'asleep', 'with', 'his', 'mouth', 'open', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2143048797384347e-231\n",
      "['dog', 'dog', 'dog', 'white', 'dog', 'jumping', 'a', 'tennis', 'ball', 'in', 'the', 'beach', '.', '.', '.', '.', '.', '.'] ['a', 'black', 'and', 'grey', 'dog', 'catches', 'a', 'tennis', 'ball', 'at', 'the', 'beach', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.345259488186094e-231\n",
      "['woman', 'girl', 'in', 'into', 'a', 'pool', 'set', 'into', 'into', 'into', '.', '.', '.', '.', '.', '.', '.', '.'] ['Young', 'boy', 'leaps', 'from', 'a', 'swing', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "8.535988047490202e-232\n",
      "['man', 'little', 'dog', 'brown', 'dog', 'is', 'playing', 'through', 'the', 'grassy', '.', 'yellow', '.', '.', '.', '.', '.', '.'] ['The', 'white', 'and', 'black', 'dog', 'is', 'running', 'through', 'a', 'field', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.3005981777288282e-231\n",
      "['two', 'man', 'is', 'rink', 'a', 'crowd', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'rollerblader', 'skating', 'inside', 'a', 'tube', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.702932729116586e-232\n",
      "['man', 'in', 'a', 'rock', 'covered', 'hill', '.', 'at', 'the', '.', 'him', '.', '.', '.', '.', '.', '.', '.'] ['Woman', 'climbing', 'a', 'snow', 'covered', 'mountain', 'looking', 'at', 'people', 'behind', 'her', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.241869770816469e-231\n",
      "['woman', 'woman', 'men', 'woman', 'conversing', 'at', 'the', 'camera', '.', 'their', '.', '.', '.', '.', '\"', '\"', '\"', '\"'] ['The', 'two', 'seated', 'girls', 'smile', 'towards', 'the', 'camera', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2219423938097297e-231\n",
      "['people', 'men', 'are', 'out', 'from', 'a', 'restaurant', 'gathering', '.', 'one', '.', '.', '.', '.', '.', '.', '\"', '\"'] ['Three', 'dogs', 'drinking', 'water', 'at', 'a', 'public', 'spigot', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.0454407583424102e-231\n",
      "['people', 'black', 'in', 'a', 'black', 'shirt', 'is', 'standing', 'in', 'a', 'of', 'a', 'crowd', 'building', '.', 'a', '.', '.'] ['A', 'man', 'in', 'a', 'white', 'hardhat', 'is', 'standing', 'in', 'front', 'of', 'a', 'green', 'truck', '.', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.4201453457534976e-231\n",
      "['hockey', 'motorcyclist', 'in', 'a', 'helmet', 'helmet', 'helmet', 'riding', 'down', 'bike', 'bike', 'down', 'a', 'track', '.', '.', '.', '.'] ['a', 'man', 'wearing', 'a', 'yellow', 'bike', 'helmet', 'walking', 'his', 'yellow', 'bike', 'along', 'a', 'sidewalk', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.3537543421525154e-231\n",
      "['boy', 'woman', 'in', 'a', 'are', 'smiling', 'at', 'a', 'outdoor', 'in', \"'\", 'train', '.', '.', 'fortune', '.', '.', '.'] ['A', 'man', 'and', 'woman', 'are', 'looking', 'at', 'an', 'exhibit', 'entitled', \"'\", 'Other', 'People', \"'s\", 'Photographs', \"'\", '.', '<PAD>']\n",
      "1.3936785333012722e-231\n",
      "['people', 'group', 'in', 'a', 'cigarette', 'to', '.', 'her', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'woman', 'blows', 'a', 'massive', 'bubble', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.50440384721771e-232\n",
      "['dog', 'dog', 'haired', 'dog', 'is', 'in', 'a', 'field', 'of', 'a', 'flowers', '.', 'a', '.', '.', '.', '.', '.'] ['A', 'long', 'haired', 'dog', 'frolics', 'in', 'a', 'meadow', 'of', 'yellow', 'flowers', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.419366116189521e-231\n",
      "['view', 'and', 'man', 'is', 'jumping', 'to', 'catch', 'in', 'a', 'air', '.', '.', '.', '.', '.', '.', '.', '.'] ['Black', 'big', 'dog', 'is', 'trying', 'to', 'chew', 'into', 'soft', 'toy', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1640469867513693e-231\n",
      "Validation Loss 2.844\n",
      "['little', 'children', 'girls', 'in', 'and', 'and', 'while', 'while', 'while', 'in', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['three', 'well', 'dressed', 'girls', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.347651015737196e-232\n",
      "['dogs', 'man', 'bird', 'is', 'a', 'pouring', 'out', 'from', 'a', 'firetruck', 'is', 'on', 'the', 'of', 'him', '.', 'him', '.', '.', '.'] ['A', 'blue', 'building', 'has', 'smoke', 'pouring', 'out', 'while', 'a', 'firetruck', 'sits', 'in', 'front', 'of', 'it', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.4285494684445393e-231\n",
      "['white', 'children', 'are', 'jumping', 'on', 'a', 'dirt', 'track', '.', 'one', '.', '.', '.', '.', '.', '.', '.', '.', '\"', '\"'] ['Four', 'motorcycles', 'are', 'racing', 'on', 'a', 'dirt', 'track', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2882297539194154e-231\n",
      "['greyhound', 'man', 'boy', 'in', 'into', 'kick', 'a', 'ball', 'from', 'a', 'game', 'game', '.', 'ankle', '.', '.', '.', '.', '.', '.'] ['A', 'young', 'boy', 'dives', 'to', 'catch', 'the', 'ball', 'during', 'a', 'baseball', 'game', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2764279323333888e-231\n",
      "['people', 'group', 'is', 'on', 'a', 'street', 'of', 'water', '.', 'a', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'dog', 'walks', 'across', 'a', 'puddle', 'of', 'water', 'in', 'Las', 'Vegas', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1988328686372911e-231\n",
      "['woman', 'woman', 'in', 'on', 'a', 'outdoor', 'with', 'a', 'dog', 'on', 'on', 'busy', 'to', 'a', 'crowd', '.', '.', '.', '.', '.'] ['A', 'man', 'sitting', 'on', 'an', 'elephant', 'with', 'a', 'riding', 'seat', 'is', 'talking', 'to', 'a', 'female', 'tourist', '.', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.292287220701382e-231\n",
      "['young', 'children', 'girls', 'in', 'a', 'suits', 'pose', 'their', '.', '.', '.', '.', '.', '.', '.', '\"', '\"', '\"', '\"', '\"'] ['Two', 'young', 'women', 'in', 'tight', 'clothing', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.157231061812019e-232\n",
      "['woman', 'woman', 'girl', 'in', 'a', 'costume', 'and', 'a', 'blue', '.', 'smiling', 'held', 'in', 'a', 'air', '.', 'her', '.', '.', '.'] ['A', 'little', 'girl', 'in', 'only', 'socks', 'and', 'a', 'necklace', 'is', 'being', 'tossed', 'in', 'the', 'air', 'and', 'caught', '.', '<PAD>', '<PAD>']\n",
      "1.4111849909420337e-231\n",
      "['man', 'man', 'player', 'is', 'a', 'ball', 'in', 'being', 'by', 'another', 'player', 'player', '.', 'the', 'other', 'team', '.', '.', '.', '.'] ['One', 'football', 'player', 'holding', 'the', 'ball', 'is', 'chased', 'by', 'another', 'football', 'player', 'from', 'the', 'opposing', 'team', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.4361312479575705e-231\n",
      "['brown', 'dogs', 'are', 'in', 'fighting', 'with', 'a', 'snow', '.', 'one', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'dogs', 'playing', 'or', 'fighting', 'in', 'the', 'snow', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2383665771889249e-231\n",
      "['black', 'woman', 'in', 'a', 'mans', 'of', 'of', 'another', 'is', 'being', 'down', '.', 'a', '.', '.', '.', '.', '.', '.', '.'] ['A', 'baby', 'touches', 'the', 'mans', 'face', 'while', 'he', 'is', 'lying', 'down', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2470187592441306e-231\n",
      "['the', 'man', 'is', 'wearing', 'a', 'helmet', 'jacket', 'and', 'he', 'is', 'a', 'mountain', 'mountain', '.', '.', '.', '.', '.', '.', '.'] ['The', 'climber', 'is', 'wearing', 'a', 'red', 'jacket', 'as', 'he', 'climbs', 'the', 'ice', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.3712934633577556e-231\n",
      "['people', 'man', 'and', 'is', 'the', 'red', 'collar', 'is', 'in', 'his', 'neck', 'is', 'sitting', 'in', 'the', 'snow', '.', 'a', 'white', '.'] ['The', 'brown', 'dog', 'with', 'a', 'blue', 'bandanna', 'tied', 'around', 'its', 'neck', 'is', 'laying', 'in', 'the', 'grass', 'by', 'the', 'water', '.']\n",
      "1.4414510309515714e-231\n",
      "['UNK', 'man', 'in', 'a', 'pink', 'hat', 'rides', 'a', 'black', 'in', 'a', 'leash', '.', 'a', '.', '.', '.', '.', '.', '.'] ['A', 'girl', 'in', 'a', 'cowboy', 'hat', 'with', 'a', 'sheep', 'on', 'a', 'leash', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2552312188307688e-231\n",
      "['people', 'crowd', 'boy', 'in', 'down', 'an', 'group', 'a', 'crowd', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'teenage', 'girl', 'walks', 'past', 'another', 'with', 'a', 'cellphone', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.181748633447778e-232\n",
      "['in', 'man', 'in', 'riding', 'up', 'snowy', 'wall', 'a', 'forest', '.', 'area', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['A', 'person', 'is', 'climbing', 'a', 'rock', 'in', 'a', 'forest', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.171080823333991e-231\n",
      "['man', 'people', 'are', 'on', 'on', 'a', 'edge', 'of', 'a', 'water', 'and', 'a', 'hillside', '.', 'their', '.', '.', '.', '.', '.'] ['Two', 'people', 'stand', 'together', 'on', 'the', 'edge', 'of', 'the', 'water', 'on', 'the', 'grass', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.2960139736913823e-231\n",
      "['man', 'men', 'are', 'a', 'are', 'a', 'top', \"'s\", 'night', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'people', 'on', 'cots', 'watching', 'the', 'water', 'at', 'twilight', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "9.893133360884868e-232\n",
      "['people', 'in', 'injured', 'hair', 'and', 'while', 'while', 'while', 'while', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Man', 'with', 'tan', 'cap', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "7.720899511627474e-232\n",
      "['black', 'men', 'girls', 'are', 'for', 'a', 'picture', 'in', 'front', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'] ['Two', 'young', 'couples', 'posing', 'for', 'a', 'picture', 'in', 'exercise', 'clothing', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "1.1988328686372911e-231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1921874587366841e-231"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics(encoder, decoder, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xkQpcmo-U_2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.load_state_dict(torch.load('pretrained_models/encoder-15.pkl'))\n",
    "# decoder.load_state_dict(torch.load('pretrained_models/decoder-15.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = 'data/test/3504479370_ff2d89a043.jpg'\n",
    "\n",
    "image = resize_image(img_path, sz=56)\n",
    "image = normalize(image)\n",
    "image = np.rollaxis(image, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = read_image(test_img_path)\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder(image)\n",
    "output = decoder.sample(features, mode=\"factual\")\n",
    "\n",
    "caption = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.topk(outputs, 1)[1].squeeze(2).data\n",
    "for i in range(batch):\n",
    "    predicted_caption = [reverse_word_map[x.item()] for x in indices[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3btvWJ5H-U_3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "stylenet_model_train_valid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
