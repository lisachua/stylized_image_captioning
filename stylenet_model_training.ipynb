{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "\n",
    "import skimage.io\n",
    "import skimage.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_7k_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_7k_images(c_type='humor'):\n",
    "    '''8k -> 7k'''\n",
    "    # open data/type/train.p\n",
    "    img_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )\n",
    "    \n",
    "    # copy imgs\n",
    "    for img_name in img_lst:\n",
    "        shutil.copyfile('data/Flicker8k_Dataset/' + img_name,\n",
    "                        'data/Flickr7k/' + img_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for first time \n",
    "select_7k_images(c_type='humor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_factual_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr8k_filename = \"data/Flickr8k_text/Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_lst = pickle.load(open( \"data/FlickrStyle_v0.9/humor/train.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames in flickr7k_images\n",
    "filenames = os.listdir('data/Flickr7k/')\n",
    "# open factual caption: Flickr8k.token.txt\n",
    "with open(flickr8k_filename, 'r') as f:\n",
    "    res = f.readlines()\n",
    "\n",
    "# write out\n",
    "with open('data/factual_train.txt', 'w') as f:\n",
    "    r = re.compile(r'#\\d*')\n",
    "    for line in res:\n",
    "        img_id = r.split(line)[0]\n",
    "        if img_id in img_id_lst:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random_select_test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_select_test_images(num=100):\n",
    "    '''select test images randomly'''\n",
    "    # get filenames in flickr7k, 30k_images\n",
    "    filenames_7k = os.listdir('data/Flickr7k/')\n",
    "    filenames_30k = os.listdir('data/Flicker8k_Dataset')\n",
    "\n",
    "    filenames = list(set(filenames_30k) - set(filenames_7k))\n",
    "    print(\"img_num: \" + str(len(filenames)))\n",
    "    random.seed(24)\n",
    "    selected = random.sample(filenames, num)\n",
    "\n",
    "    # copy images\n",
    "    for img_name in selected:\n",
    "        shutil.copyfile('data/Flicker8k_Dataset/' + img_name,\n",
    "                        'data/test_images/' + img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for first time\n",
    "random_select_test_images(num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    '''vocabulary'''\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.ix = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.w2i:\n",
    "            self.w2i[word] = self.ix\n",
    "            self.i2w[self.ix] = word\n",
    "            self.ix += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if word not in self.w2i:\n",
    "            return self.w2i['<unk>']\n",
    "        return self.w2i[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(mode_list=['factual', 'humorous']):\n",
    "    '''build vocabulary'''\n",
    "    # define vocabulary\n",
    "    vocab = Vocab()\n",
    "    # add special tokens\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<s>')\n",
    "    vocab.add_word('</s>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # add words\n",
    "    for mode in mode_list:\n",
    "        if mode == 'factual':\n",
    "            captions = extract_captions(mode=mode)\n",
    "            words = nltk.tokenize.word_tokenize(captions)\n",
    "            counter = Counter(words)\n",
    "            words = [word for word, cnt in counter.items() if cnt >= 2]\n",
    "        else:\n",
    "            captions = extract_captions(mode=mode)\n",
    "            words = nltk.tokenize.word_tokenize(captions)\n",
    "\n",
    "        for word in words:\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/factual_train.txt\", 'r') as f:\n",
    "    res = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_captions(mode='factual'):\n",
    "    '''extract captions from data files for building vocabulary'''\n",
    "    text = ''\n",
    "    if mode == 'factual':\n",
    "        with open(\"data/factual_train.txt\", 'r') as f:\n",
    "            res = f.readlines()\n",
    "\n",
    "    elif mode == 'humorous':\n",
    "        with open(\"data/FlickrStyle_v0.9/humor/funny_train.txt\", 'r') as f:\n",
    "            res = f.readlines()\n",
    "    else:\n",
    "        with open(\"data/FlickrStyle_v0.9/romantic/romantic_train.txt\", 'r') as f:\n",
    "            res = f.readlines()\n",
    "\n",
    "    for line in res:\n",
    "        line = line.replace('.', '')\n",
    "        line = line.strip()\n",
    "        text += line + ' '\n",
    "\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14889\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(mode_list=['factual', 'humorous'])\n",
    "print(vocab.__len__())\n",
    "with open('data/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Flickr7kDataset(Dataset):\n",
    "    '''Flickr7k dataset'''\n",
    "    def __init__(self, img_dir, caption_file, vocab, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            img_dir: Direcutory with all the images\n",
    "            caption_file: Path to the factual caption file\n",
    "            vocab: Vocab instance\n",
    "            transform: Optional transform to be applied\n",
    "        '''\n",
    "        self.img_dir = img_dir\n",
    "        self.imgname_caption_list = self._get_imgname_and_caption(caption_file)\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def _get_imgname_and_caption(self, caption_file):\n",
    "        '''extract image name and caption from factual caption file'''\n",
    "        with open(caption_file, 'r') as f:\n",
    "            res = f.readlines()\n",
    "\n",
    "        imgname_caption_list = []\n",
    "        r = re.compile(r'#\\d*')\n",
    "        for line in res:\n",
    "            img_and_cap = r.split(line)\n",
    "            img_and_cap = [x.strip() for x in img_and_cap]\n",
    "            imgname_caption_list.append(img_and_cap)\n",
    "\n",
    "        return imgname_caption_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgname_caption_list)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        '''return one data pair (image and captioin)'''\n",
    "        img_name = self.imgname_caption_list[ix][0]\n",
    "        img_name = os.path.join(self.img_dir, img_name)\n",
    "        caption = self.imgname_caption_list[ix][1]\n",
    "\n",
    "        image = skimage.io.imread(img_name)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # convert caption to word ids\n",
    "        r = re.compile(\"\\.\")\n",
    "        tokens = nltk.tokenize.word_tokenize(r.sub(\"\", caption).lower())\n",
    "        caption = []\n",
    "        caption.append(self.vocab('<s>'))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab('</s>'))\n",
    "        caption = torch.Tensor(caption)\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    '''create minibatch tensors from data(list of tuple(image, caption))'''\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # images : tuple of 3D tensor -> 4D tensor\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # captions : tuple of 1D Tensor -> 2D tensor\n",
    "    lengths = torch.LongTensor([len(cap) for cap in captions])\n",
    "    captions = [pad_sequence_dl(cap, max(lengths)) for cap in captions]\n",
    "    captions = torch.stack(captions, 0)\n",
    "\n",
    "    return images, captions, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_dl(seq, max_len):\n",
    "    seq = torch.cat((seq, torch.zeros(max_len - len(seq))))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(img_dir, caption_file, vocab, batch_size,\n",
    "                    transform=None, shuffle=False, num_workers=0):\n",
    "    '''Return data_loader'''\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            Rescale((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    flickr7k = Flickr7kDataset(img_dir, caption_file, vocab, transform)\n",
    "\n",
    "    data_loader = DataLoader(dataset=flickr7k,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers,\n",
    "                             collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrStyle7kDataset(Dataset):\n",
    "    '''Styled caption dataset'''\n",
    "    def __init__(self, caption_file, vocab):\n",
    "        '''\n",
    "        Args:\n",
    "            caption_file: Path to styled caption file\n",
    "            vocab: Vocab instance\n",
    "        '''\n",
    "        self.caption_list = self._get_caption(caption_file)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def _get_caption(self, caption_file):\n",
    "        '''extract caption list from styled caption file'''\n",
    "        with open(caption_file, 'r') as f:\n",
    "            caption_list = f.readlines()\n",
    "\n",
    "        caption_list = [x.strip() for x in caption_list]\n",
    "        return caption_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.caption_list)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        caption = self.caption_list[ix]\n",
    "        # convert caption to word ids\n",
    "        r = re.compile(\"\\.\")\n",
    "        tokens = nltk.tokenize.word_tokenize(r.sub(\"\", caption).lower())\n",
    "        caption = []\n",
    "        caption.append(self.vocab('<s>'))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab('</s>'))\n",
    "        caption = torch.Tensor(caption)\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_styled(captions):\n",
    "    captions.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "    # tuple of 1D Tensor -> 2D Tensor\n",
    "    lengths = torch.LongTensor([len(cap) for cap in captions])\n",
    "    captions = [pad_sequence_dl(cap, max(lengths)) for cap in captions]\n",
    "    captions = torch.stack(captions, 0)\n",
    "\n",
    "    return captions, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_styled_data_loader(caption_file, vocab, batch_size,\n",
    "                           shuffle=False, num_workers=0):\n",
    "    '''Return data_loader for styled caption'''\n",
    "    flickr_styled_7k = FlickrStyle7kDataset(caption_file, vocab)\n",
    "\n",
    "    data_loader = DataLoader(dataset=flickr_styled_7k,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=num_workers,\n",
    "                             collate_fn=collate_fn_styled)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale:\n",
    "    '''Rescale the image to a given size\n",
    "    Args:\n",
    "        output_size(int or tuple)\n",
    "    '''\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        image = skimage.transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/vocab.pkl\", 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "img_path = \"data/Flickr7k\"\n",
    "cap_path = \"data/factual_train.txt\"\n",
    "cap_path_styled = \"data/FlickrStyle_v0.9/humor/funny_train.txt\"\n",
    "data_loader = get_data_loader(img_path, cap_path, vocab, 3)\n",
    "styled_data_loader = get_styled_data_loader(cap_path_styled, vocab, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (captions, lengths) in enumerate(styled_data_loader):\n",
    "    print(i)\n",
    "    # print(images.shape)\n",
    "    print(captions[:, 1:])\n",
    "    print(lengths - 1)\n",
    "    print()\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.range(0, max_len - 1).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def masked_cross_entropy(logits, target, length):\n",
    "    length = Variable(length)\n",
    "    if torch.cuda.is_available():\n",
    "        length = length.cuda()\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "         False, False, False]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "length = torch.LongTensor([23, 21, 17])\n",
    "\n",
    "print(sequence_mask(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from constant import get_symbol_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        '''\n",
    "        Load the pretrained ResNet152 and replace fc\n",
    "        '''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.A = nn.Linear(resnet.fc.in_features, emb_dim)\n",
    "\n",
    "    def forward(self, images):\n",
    "        '''Extract the image feature vectors'''\n",
    "        features = self.resnet(images)\n",
    "        features = Variable(features.data)\n",
    "        if torch.cuda.is_available():\n",
    "            features = features.cuda()\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.A(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactoredLSTM(nn.Module):\n",
    "    def __init__(self, emb_dim, hidden_dim, factored_dim,  vocab_size):\n",
    "        super(FactoredLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # embedding\n",
    "        self.B = nn.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "        # factored lstm weights\n",
    "        self.U_i = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fi = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_i = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_i = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_f = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_ff = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_f = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_f = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_o = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fo = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_o = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.U_c = nn.Linear(factored_dim, hidden_dim)\n",
    "        self.S_fc = nn.Linear(factored_dim, factored_dim)\n",
    "        self.V_c = nn.Linear(emb_dim, factored_dim)\n",
    "        self.W_c = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # h - humorous\n",
    "        self.S_hi = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_hf = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_ho = nn.Linear(factored_dim, factored_dim)\n",
    "        self.S_hc = nn.Linear(factored_dim, factored_dim)\n",
    "\n",
    "        # r - romantic\n",
    "        # self.S_ri = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_rf = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_ro = nn.Linear(factored_dim, factored_dim)\n",
    "        # self.S_rc = nn.Linear(factored_dim, factored_dim)\n",
    "\n",
    "        # weight for output\n",
    "        self.C = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward_step(self, embedded, h_0, c_0, mode):\n",
    "        # transform embedded from emb_dim --> factored_dim\n",
    "        i = self.V_i(embedded)\n",
    "        f = self.V_f(embedded)\n",
    "        o = self.V_o(embedded)\n",
    "        c = self.V_c(embedded)\n",
    "        \n",
    "        # factored_dim --> factored_dim\n",
    "        if mode == \"factual\":\n",
    "            i = self.S_fi(i)\n",
    "            f = self.S_ff(f)\n",
    "            o = self.S_fo(o)\n",
    "            c = self.S_fc(c)\n",
    "        elif mode == \"humorous\":\n",
    "            i = self.S_hi(i)\n",
    "            f = self.S_hf(f)\n",
    "            o = self.S_ho(o)\n",
    "            c = self.S_hc(c)\n",
    "        # elif mode == \"romantic\":\n",
    "        #     i = self.S_ri(i)\n",
    "        #     f = self.S_rf(f)\n",
    "        #     o = self.S_ro(o)\n",
    "        #     c = self.S_rc(c)\n",
    "        else:\n",
    "            sys.stderr.write(\"mode name wrong!\")\n",
    "\n",
    "        i_t = F.sigmoid(self.U_i(i.double()) + self.W_i(h_0.double()))\n",
    "        f_t = F.sigmoid(self.U_f(f.double()) + self.W_f(h_0.double()))\n",
    "        o_t = F.sigmoid(self.U_o(o.double()) + self.W_o(h_0.double()))\n",
    "        c_tilda = F.tanh(self.U_c(c.double()) + self.W_c(h_0.double()))\n",
    "\n",
    "        c_t = f_t * c_0 + i_t * c_tilda\n",
    "        h_t = o_t * c_t\n",
    "\n",
    "        outputs = self.C(h_t)\n",
    "\n",
    "        return outputs, h_t, c_t\n",
    "\n",
    "    def forward(self, captions, features=None, mode=\"factual\"):\n",
    "        '''\n",
    "        Args:\n",
    "            features: fixed vectors from images, [batch, emb_dim]\n",
    "            captions: [batch, max_len]\n",
    "            mode: type of caption to generate\n",
    "        '''\n",
    "        batch_size = captions.size(0)\n",
    "        embedded = self.B(captions)  # [batch, max_len, emb_dim]\n",
    "        # concat image features and captions\n",
    "        if mode == \"factual\":\n",
    "            if features is None:\n",
    "                sys.stderr.write(\"features is None!\")\n",
    "            embedded = torch.cat((features.unsqueeze(1), embedded), 1)\n",
    "\n",
    "        # initialize hidden state\n",
    "        h_t = Variable(torch.Tensor(batch_size, self.hidden_dim))\n",
    "        c_t = Variable(torch.Tensor(batch_size, self.hidden_dim))\n",
    "        nn.init.uniform(h_t)\n",
    "        nn.init.uniform(c_t)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h_t = h_t.cuda()\n",
    "            c_t = c_t.cuda()\n",
    "\n",
    "        all_outputs = []\n",
    "        # iterate\n",
    "        for ix in range(embedded.size(1) - 1):\n",
    "            emb = embedded[:, ix, :]\n",
    "            outputs, h_t, c_t = self.forward_step(emb, h_t, c_t, mode=mode)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "        all_outputs = torch.stack(all_outputs, 1)\n",
    "\n",
    "        return all_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "def eval_outputs(outputs, vocab):\n",
    "    # outputs: [batch, max_len - 1, vocab_size]\n",
    "    indices = torch.topk(outputs, 1)[1]\n",
    "    indices = indices.squeeze(2)\n",
    "    indices = indices.data\n",
    "    for i in range(len(indices)):\n",
    "        caption = [vocab.i2w[x] for x in indices[i]]\n",
    "        print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "data_loader = get_data_loader(img_path, cap_path, vocab, batch_size)\n",
    "styled_data_loader = get_styled_data_loader(cap_path_styled, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "hidden_dim = 512\n",
    "factored_dim = 512\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14889"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(emb_dim)\n",
    "decoder = FactoredLSTM(emb_dim, hidden_dim, factored_dim, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_caption = 0.0002\n",
    "lr_language = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = masked_cross_entropy\n",
    "cap_params = list(decoder.parameters()) + list(encoder.A.parameters())\n",
    "lang_params = list(decoder.parameters())\n",
    "optimizer_cap = torch.optim.Adam(cap_params, lr=lr_caption)\n",
    "optimizer_lang = torch.optim.Adam(lang_params, lr=lr_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "total_cap_step = len(data_loader)\n",
    "total_lang_step = len(styled_data_loader)\n",
    "epoch_num = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'pretrained_models'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_step_caption = 50\n",
    "log_step_language = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.double()\n",
    "decoder = decoder.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_outputs(outputs, vocab):\n",
    "    # outputs: [batch, max_len - 1, vocab_size]\n",
    "    indices = torch.topk(outputs, 1)[1]\n",
    "    indices = indices.squeeze(2)\n",
    "    indices = indices.data\n",
    "    for i in range(len(indices)):\n",
    "        caption = [vocab.i2w[x.item()] for x in indices[i]]\n",
    "        print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:102: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:103: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Users/eliu/opt/anaconda3/envs/usf/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], CAP, Step [0/700], Loss: 9.6081\n",
      "Epoch [1/1], CAP, Step [0/700], Loss: 9.6081\n",
      "Epoch [1/1], CAP, Step [1/700], Loss: 9.5723\n",
      "Epoch [1/1], CAP, Step [2/700], Loss: 9.5490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b1c9714b2dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"factual\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         loss = criterion(outputs[:, 1:, :].contiguous(),\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-eb0f4b560886>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;34m'''Extract the image feature vectors'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/usf/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    # caption\n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "#         images = to_var(images, volatile=True)\n",
    "#         captions = to_var(captions.long())\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            captions = captions.cuda()\n",
    "            \n",
    "        # forward, backward and optimize\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(captions.long(), features.double(), mode=\"factual\")\n",
    "        loss = criterion(outputs[:, 1:, :].contiguous(),\n",
    "                         captions[:, 1:].contiguous().long(), lengths - 1)\n",
    "        loss.backward()\n",
    "        optimizer_cap.step()\n",
    "        \n",
    "        print(\"Epoch [%d/%d], CAP, Step [%d/%d], Loss: %.4f\"\n",
    "                  % (epoch+1, epoch_num, i, total_cap_step,\n",
    "                      loss.data.mean()))\n",
    "        \n",
    "        #if i == 3: break\n",
    "\n",
    "        # print log\n",
    "        if i % log_step_caption == 0:\n",
    "            print(\"Epoch [%d/%d], CAP, Step [%d/%d], Loss: %.4f\"\n",
    "                  % (epoch+1, epoch_num, i, total_cap_step,\n",
    "                      loss.data.mean()))\n",
    "\n",
    "    eval_outputs(outputs, vocab)\n",
    "\n",
    "    # language\n",
    "    for i, (captions, lengths) in enumerate(styled_data_loader):\n",
    "        #captions = to_var(captions.long())\n",
    "        if torch.cuda.is_available():\n",
    "            captions = captions.cuda()\n",
    "\n",
    "        # forward, backward and optimize\n",
    "        decoder.zero_grad()\n",
    "        outputs = decoder(captions.long(), mode='humorous')\n",
    "        loss = criterion(outputs, captions[:, 1:].contiguous().long(), lengths-1)\n",
    "        loss.backward()\n",
    "        optimizer_lang.step()\n",
    "\n",
    "        # print log\n",
    "        if i % log_step_language == 0:\n",
    "            print(\"Epoch [%d/%d], LANG, Step [%d/%d], Loss: %.4f\"\n",
    "                  % (epoch+1, epoch_num, i, total_lang_step,\n",
    "                      loss.data.mean()))\n",
    "\n",
    "#         print(\"Epoch [%d/%d], LANG, Step [%d/%d], Loss: %.4f\"\n",
    "#                   % (epoch+1, epoch_num, i, total_lang_step,\n",
    "#                       loss.data.mean()))\n",
    "        #if i == 3: break\n",
    "\n",
    "    # save models\n",
    "    torch.save(decoder.state_dict(),\n",
    "               os.path.join(model_path, 'decoder-%d.pkl' % (epoch + 1,)))\n",
    "\n",
    "    torch.save(encoder.state_dict(),\n",
    "               os.path.join(model_path, 'encoder-%d.pkl' % (epoch + 1,)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument('--caption_batch_size', type=int, default=64,\n",
    "                        help='mini batch size for caption model training')\n",
    "    parser.add_argument('--language_batch_size', type=int, default=96,\n",
    "                        help='mini batch size for language model training')\n",
    "    parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                        help='embedding size of word, image')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=512,\n",
    "                        help='hidden state size of factored LSTM')\n",
    "    parser.add_argument('--factored_dim', type=int, default=512,\n",
    "                        help='size of factored matrix')\n",
    "    parser.add_argument('--lr_caption', type=int, default=0.0002,\n",
    "                        help='learning rate for caption model training')\n",
    "    parser.add_argument('--lr_language', type=int, default=0.0005,\n",
    "                        help='learning rate for language model training')\n",
    "    parser.add_argument('--epoch_num', type=int, default=30)\n",
    "    parser.add_argument('--log_step_caption', type=int, default=50,\n",
    "                        help='steps for print log while train caption model')\n",
    "    parser.add_argument('--log_step_language', type=int, default=10,\n",
    "                        help='steps for print log while train language model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
