{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from itertools import chain\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "Dataset: Flickr8k https://www.kaggle.com/shadabhussain/flickr8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg#0\tA child in a pink dress is climbing up a set of stairs in an entry way .\n",
      "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden building .\n",
      "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a wooden playhouse .\n",
      "1000268201_693b08cb0e.jpg#3\tA little girl climbing the s\n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "filename = \"Flickr8k_text/Flickr8k.token.txt\"\n",
    "doc = load_doc(filename)\n",
    "print(doc[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_descriptions(doc):\n",
    "    image_id_lst = []\n",
    "    caption_no_lst = []\n",
    "    image_desc_lst = []\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id_no, image_desc = tokens[0], tokens[1:]\n",
    "        # extract filename from image id\n",
    "        image_id  = image_id_no.split('#')[0]\n",
    "        caption_no = image_id_no.split('#')[1]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        image_id_lst.append(image_id)\n",
    "        caption_no_lst.append(caption_no)\n",
    "        image_desc_lst.append(image_desc)\n",
    "        df = pd.DataFrame(list(zip(image_id_lst, caption_no_lst, image_desc_lst)))\n",
    "    return df\n",
    "\n",
    "# parse descriptions\n",
    "df = load_descriptions(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0  1  \\\n",
       "0  1000268201_693b08cb0e.jpg  0   \n",
       "1  1000268201_693b08cb0e.jpg  1   \n",
       "2  1000268201_693b08cb0e.jpg  2   \n",
       "3  1000268201_693b08cb0e.jpg  3   \n",
       "4  1000268201_693b08cb0e.jpg  4   \n",
       "\n",
       "                                                   2  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating PyTorch Dataset\n",
    "The below code is based on https://github.com/Mdhvince/Image_Captioning and https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    \"\"\"Image Caption dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df, root_dir, mapper_file, max_seq_length=20, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        print(\"Reading data...\")\n",
    "        self.df = df\n",
    "        self.captions_column = self.df['captions']\n",
    "        self.img_name_column = self.df['img_name']\n",
    "        \n",
    "        print(\"Calculating length...\")\n",
    "        self.df['length'] = self.captions_column.apply(lambda x: len(x.split()))\n",
    "        self.length_column = self.df['length']\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        print(\"Reading Mapper file...\")\n",
    "        with open('mapping.pkl', 'rb') as f:\n",
    "            self.mapper_file = pickle.load(f)\n",
    "        \n",
    "        print(\"Ready !\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # take the image name contained in the csv file\n",
    "        image_name = os.path.join(self.root_dir, self.img_name_column[idx])\n",
    "        \n",
    "        #image_name = os.path.join(self.root_dir,\n",
    "                                  #self.df.iloc[idx, 0])\n",
    "        \n",
    "\n",
    "        # read the true image based on that name\n",
    "        # choice: mpimg because done with 1 line\n",
    "        # with cv2, I need to read the convert from BGR2RGB\n",
    "        image = mpimg.imread(image_name)\n",
    "        \n",
    "        # read df & transform caption to tensor\n",
    "        caption = self.captions_column[idx]\n",
    "        #caption = self.df.iloc[idx, 1]\n",
    "        caption = caption.lower()\n",
    "        tokens = word_tokenize(caption)\n",
    "                \n",
    "        caption = []\n",
    "        caption.append('<start>')\n",
    "        caption.extend([token for token in tokens])\n",
    "        caption.append('<end>')\n",
    "        \n",
    "        # Map to integer\n",
    "        caption = [self.mapper_file[i] for i in caption]\n",
    "        \n",
    "        #pad sequence\n",
    "        caption = self.pad_data(caption)\n",
    "        \n",
    "        sample = {'image': image, 'caption': caption}\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pad_data(self, s):\n",
    "        padded = np.ones((self.max_seq_length,), dtype=np.int64)*self.mapper_file['<PAD>']\n",
    "        \n",
    "        if len(s) > self.max_seq_length:\n",
    "            padded[:] = s[:self.max_seq_length]\n",
    "        else: \n",
    "            padded[:len(s)] = s\n",
    "            \n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, root_dir, mapper_file):\n",
    "    with open(mapper_file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        Rescale(224),\n",
    "        Normalize(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    train_set = ImageCaptionDataset(df=df,\n",
    "                                    root_dir=root_dir,\n",
    "                                    mapper_file=mapper_file,\n",
    "                                    transform=transform)\n",
    "    return train_set, vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Valid Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(training_set, validation_size):\n",
    "    \"\"\" Function that split our dataset into train and validation\n",
    "        given in parameter the training set and the % of sample for validation\"\"\"\n",
    "    \n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(training_set)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(validation_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    return train_sampler, valid_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler, valid_sampler = train_valid_split(train_set, valid_size)\n",
    "train_loader = DataLoader(train_set,\n",
    "                            batch_size=batch_size,\n",
    "                            sampler=train_sampler,\n",
    "                            num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                            batch_size=batch_size,\n",
    "                                            sampler=valid_sampler,\n",
    "                                            num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Image Encoder using ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        #import pre trained model\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # remove last fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        \n",
    "        # build the new resnet\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # our additional Fully connected layer with an output = the embbed size\n",
    "        # to feed the rnn\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        #call resnet on our images\n",
    "        features = self.resnet(images)\n",
    "        \n",
    "        #flatten for our additional fc layer\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        features = self.embed(features)\n",
    "        \n",
    "        return features #here is our spacial information extracted from the image with the right output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Decoder to generate captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_size\n",
    "        \n",
    "        # Our embedding layer\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embed_size, self.hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # The linear layer maps the hidden state output of the LSTM to the number of words we want:\n",
    "        # vocab_size\n",
    "        self.linear = nn.Linear(self.hidden_dim, vocab_size)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" \n",
    "        Here we need to define h0, c0 with all zeroes in order to initialize our LSTM\n",
    "        Architecture\n",
    "        \"\"\"\n",
    "        return torch.zeros((1, batch_size, self.hidden_dim)), torch.zeros((1, batch_size, self.hidden_dim))\n",
    "    \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        # Make sure that features shape are :batch_size, embed_size\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Initialize the hidden state\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Create embedded word vectors for each word in the captions\n",
    "        embeddings = self.word_embeddings(captions)\n",
    "        \n",
    "        # Stack the features and captions\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1) \n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embeddings, self.hidden) \n",
    "        \n",
    "        out = self.linear(lstm_out)\n",
    "        \n",
    "        out = out[:, :-1]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# This is to make sure that the 1st loss is  lower than sth and\n",
    "# Save the model according to this comparison\n",
    "valid_loss_min = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # Keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for data in train_loader:\n",
    "        images, captions = data['image'], data['caption']\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        images.to(device)\n",
    "        captions.to(device)\n",
    "\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.view(-1))\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for data in valid_loader:\n",
    "        images, captions = data['image'], data['caption']\n",
    "        images = images.type(torch.FloatTensor)\n",
    "        images.to(device)\n",
    "        captions.to(device)\n",
    "\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        loss = criterion(outputs.contiguous().view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "\n",
    "        # Average losses\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "\n",
    "        print(f\"Epoch: {epoch} \\tTraining Loss: {train_loss} \\tValidation Loss: {valid_loss}\")\n",
    "\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f\"Validation loss decreased ({valid_loss_min} --> {valid_loss}).  Saving model ...\")\n",
    "            torch.save(encoder.state_dict(), save_location_path+'/encoder{n_epochs}.pt')\n",
    "            torch.save(decoder.state_dict(), save_location_path+'/decoder{n_epochs}.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
